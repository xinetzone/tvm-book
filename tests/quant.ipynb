{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试自动量化\n",
    "\n",
    "## 模型导入\n",
    "\n",
    "以 PyTorch 前端为例阐述 TVM 自动量化机制。\n",
    "\n",
    "创建单层卷积："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv = nn.Conv2d(3, 16, 3, 1, 1)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TVM 接受 {func}`torch.jit.trace` 后的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* span=aten___convolution_0_data:0:0 */, %aten___convolution_0_weight: Tensor[(16, 3, 3, 3), float32] /* span=aten___convolution_0_weight:0:0 */, %aten___convolution_0_bias: Tensor[(16), float32] /* span=aten___convolution_0_bias:0:0 */, %aten__batch_norm_0_weight: Tensor[(16), float32] /* span=aten__batch_norm_0_weight:0:0 */, %aten__batch_norm_0_bias: Tensor[(16), float32] /* span=aten__batch_norm_0_bias:0:0 */, %aten__batch_norm_0_mean: Tensor[(16), float32] /* span=aten__batch_norm_0_mean:0:0 */, %aten__batch_norm_0_var: Tensor[(16), float32] /* span=aten__batch_norm_0_var:0:0 */) {\n",
      "  %0 = nn.conv2d(%data, %aten___convolution_0_weight, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, %aten___convolution_0_bias) /* span=aten___convolution_0:0:0 */;\n",
      "  %2 = nn.batch_norm(%1, %aten__batch_norm_0_weight, %aten__batch_norm_0_bias, %aten__batch_norm_0_mean, %aten__batch_norm_0_var) /* span=aten__batch_norm_0:0:0 */;\n",
      "  %3 = %2.0 /* span=aten__batch_norm_0:0:0 */;\n",
      "  nn.relu(%3) /* span=aten__relu_0:0:0 */\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tvm import relay\n",
    "\n",
    "pt_model = Model().eval().float()\n",
    "ishape = (1, 3, 4, 4)\n",
    "input_shapes = [(\"data\", ishape)]\n",
    "# script_module = torch.jit.script(pt_model)\n",
    "# mod, params = relay.frontend.from_pytorch(script_module, input_shapes)\n",
    "idata = torch.rand(ishape).type(torch.float32)\n",
    "traced_model = torch.jit.trace(pt_model, idata)\n",
    "# traced_model 翻译为 TVM 前端模型\n",
    "mod, params = relay.frontend.from_pytorch(traced_model, input_shapes, use_parser_friendly_name=True)\n",
    "print(mod[\"main\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化 TVM 模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = multiply(%data, 16f /* ty=float32 */) /* ty=Tensor[(1, 3, 4, 4), float32] */;\n",
      "  %1 = round(%0) /* ty=Tensor[(1, 3, 4, 4), float32] */;\n",
      "  %2 = clip(%1, a_min=-127f, a_max=127f) /* ty=Tensor[(1, 3, 4, 4), float32] */;\n",
      "  %3 = cast(%2, dtype=\"int8\") /* ty=Tensor[(1, 3, 4, 4), int8] */;\n",
      "  %4 = nn.conv2d(%3, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), int8] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3], out_dtype=\"int32\") /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %5 = add(%4, meta[relay.Constant][1] /* ty=Tensor[(16, 1, 1), int32] */) /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %6 = add(%5, meta[relay.Constant][2] /* ty=Tensor[(16, 1, 1), int32] */) /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %7 = nn.relu(%6) /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %8 = add(%7, 256 /* ty=int32 */) /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %9 = right_shift(%8, 9 /* ty=int32 */) /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %10 = clip(%9, a_min=-127f, a_max=127f) /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %11 = cast(%10, dtype=\"int8\") /* ty=Tensor[(1, 16, 4, 4), int8] */;\n",
      "  %12 = annotation.stop_fusion(%11) /* ty=Tensor[(1, 16, 4, 4), int8] */;\n",
      "  %13 = cast(%12, dtype=\"float32\") /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  multiply(%13, 0.0625f /* ty=float32 */) /* ty=Tensor[(1, 16, 4, 4), float32] */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    with relay.quantize.qconfig(skip_conv_layers=[]):\n",
    "        qmod = relay.quantize.quantize(mod, params)\n",
    "print(qmod[\"main\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dev = tvm.cpu()\n",
    "data_np = np.random.uniform(low=-1, high=1, size=[1, 3, 4, 4]).astype(\"float32\")\n",
    "input_dict = {\"data\": data_np}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化前结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    vm_exec = relay.vm.compile(mod, target=\"llvm\", params=params)\n",
    "vm = tvm.runtime.vm.VirtualMachine(vm_exec, dev)\n",
    "vm.set_input(\"main\", **input_dict)\n",
    "tvm_res = vm.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化后结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    qvm_exec = relay.vm.compile(qmod, target=\"llvm\", params=params)\n",
    "qvm = tvm.runtime.vm.VirtualMachine(qvm_exec, dev)\n",
    "qvm.set_input(\"main\", **input_dict)\n",
    "tvm_qres = qvm.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比 Torch 结果与 TVM 浮点结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch_res = traced_model(torch.from_numpy(data_np))\n",
    "np.testing.assert_allclose(\n",
    "    tvm_res.numpy(), torch_res.numpy(),\n",
    "    rtol=1e-5, atol=1e-5\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看量化前后的余弦相似度与 $L2$ 损失："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm_book.testing.metric import cosine_similarity, l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9974910020828247, 0.0001684804738033563)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    cosine_similarity(tvm_res.numpy(), tvm_qres.numpy()), \n",
    "    l2_loss(tvm_res.numpy(), tvm_qres.numpy())\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 源码解析\n",
    "\n",
    "可以打印完整的量化流程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行 pass: The meta data of the pass - pass name: sequential, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* span=aten___convolution_0_data:0:0 */) {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0], padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1]) /* span=aten___convolution_0:0:0 */;\n",
      "  %2 = nn.batch_norm(%1, meta[relay.Constant][2], meta[relay.Constant][3], meta[relay.Constant][4], meta[relay.Constant][5]) /* span=aten__batch_norm_0:0:0 */;\n",
      "  %3 = %2.0 /* span=aten__batch_norm_0:0:0 */;\n",
      "  nn.relu(%3) /* span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> meta[IncompleteType][0] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* span=aten___convolution_0_data:0:0 */) {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0], padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1]) /* span=aten___convolution_0:0:0 */;\n",
      "  %2 = nn.batch_norm(%1, meta[relay.Constant][2], meta[relay.Constant][3], meta[relay.Constant][4], meta[relay.Constant][5]) /* span=aten__batch_norm_0:0:0 */;\n",
      "  %3 = %2.0 /* span=aten__batch_norm_0:0:0 */;\n",
      "  nn.relu(%3) /* span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> meta[IncompleteType][0] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: SimplifyInference, opt_level: 0, required passes: [\n",
      "InferType, ]\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %2 = nn.batch_norm(%1, meta[relay.Constant][2] /* ty=Tensor[(16), float32] */, meta[relay.Constant][3] /* ty=Tensor[(16), float32] */, meta[relay.Constant][4] /* ty=Tensor[(16), float32] */, meta[relay.Constant][5] /* ty=Tensor[(16), float32] */) /* ty=(Tensor[(1, 16, 4, 4), float32], Tensor[(16), float32], Tensor[(16), float32]) span=aten__batch_norm_0:0:0 */;\n",
      "  %3 = %2.0 /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__batch_norm_0:0:0 */;\n",
      "  nn.relu(%3) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = add(meta[relay.Constant][2] /* ty=Tensor[(16), float32] */, 1e-05f);\n",
      "  %2 = sqrt(%1);\n",
      "  %3 = divide(1f, %2);\n",
      "  %4 = multiply(%3, meta[relay.Constant][3] /* ty=Tensor[(16), float32] */);\n",
      "  %5 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %6 = expand_dims(%4, axis=1, num_newaxis=2);\n",
      "  %7 = negative(meta[relay.Constant][4] /* ty=Tensor[(16), float32] */);\n",
      "  %8 = multiply(%7, %4);\n",
      "  %9 = add(%8, meta[relay.Constant][5] /* ty=Tensor[(16), float32] */);\n",
      "  %10 = multiply(%5, %6);\n",
      "  %11 = expand_dims(%9, axis=1, num_newaxis=2);\n",
      "  %12 = add(%10, %11);\n",
      "  nn.relu(%12) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: FoldConstant, opt_level: 2, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = add(meta[relay.Constant][2] /* ty=Tensor[(16), float32] */, 1e-05f /* ty=float32 */) /* ty=Tensor[(16), float32] */;\n",
      "  %2 = sqrt(%1) /* ty=Tensor[(16), float32] */;\n",
      "  %3 = divide(1f /* ty=float32 */, %2) /* ty=Tensor[(16), float32] */;\n",
      "  %4 = multiply(%3, meta[relay.Constant][3] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(16), float32] */;\n",
      "  %5 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %6 = expand_dims(%4, axis=1, num_newaxis=2) /* ty=Tensor[(16, 1, 1), float32] */;\n",
      "  %7 = negative(meta[relay.Constant][4] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(16), float32] */;\n",
      "  %8 = multiply(%7, %4) /* ty=Tensor[(16), float32] */;\n",
      "  %9 = add(%8, meta[relay.Constant][5] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(16), float32] */;\n",
      "  %10 = multiply(%5, %6) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %11 = expand_dims(%9, axis=1, num_newaxis=2) /* ty=Tensor[(16, 1, 1), float32] */;\n",
      "  %12 = add(%10, %11) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%12) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %2 = multiply(%1, meta[relay.Constant][2]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %3 = add(%2, meta[relay.Constant][3]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%3) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: FoldScaleAxis, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %2 = multiply(%1, meta[relay.Constant][2] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %3 = add(%2, meta[relay.Constant][3] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%3) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %2 = multiply(%1, meta[relay.Constant][2] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %3 = add(%2, meta[relay.Constant][3] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%3) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: BackwardFoldScaleAxis, opt_level: 3, required passes: [\n",
      "InferType, ]\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten___convolution_0:0:0 */;\n",
      "  %2 = multiply(%1, meta[relay.Constant][2] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %3 = add(%2, meta[relay.Constant][3] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%3) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = squeeze(meta[relay.Constant][1] /* ty=Tensor[(16, 1, 1), float32] */, axis=[1, 2]);\n",
      "  %1 = expand_dims(%0, axis=1, num_newaxis=3);\n",
      "  %2 = multiply(meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, %1);\n",
      "  %3 = nn.conv2d(%data, %2, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]);\n",
      "  %4 = multiply(meta[relay.Constant][2] /* ty=Tensor[(16), float32] */, %0);\n",
      "  %5 = nn.bias_add(%3, %4);\n",
      "  %6 = add(%5, meta[relay.Constant][3] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%6) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = squeeze(meta[relay.Constant][1] /* ty=Tensor[(16, 1, 1), float32] */, axis=[1, 2]) /* ty=Tensor[(16), float32] */;\n",
      "  %1 = expand_dims(%0, axis=1, num_newaxis=3) /* ty=Tensor[(16, 1, 1, 1), float32] */;\n",
      "  %2 = multiply(meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, %1) /* ty=Tensor[(16, 3, 3, 3), float32] */;\n",
      "  %3 = nn.conv2d(%data, %2, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %4 = multiply(meta[relay.Constant][2] /* ty=Tensor[(16), float32] */, %0) /* ty=Tensor[(16), float32] */;\n",
      "  %5 = nn.bias_add(%3, %4) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %6 = add(%5, meta[relay.Constant][3] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%6) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: ForwardFoldScaleAxis, opt_level: 3, required passes: [\n",
      "InferType, ]\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = squeeze(meta[relay.Constant][1] /* ty=Tensor[(16, 1, 1), float32] */, axis=[1, 2]) /* ty=Tensor[(16), float32] */;\n",
      "  %1 = expand_dims(%0, axis=1, num_newaxis=3) /* ty=Tensor[(16, 1, 1, 1), float32] */;\n",
      "  %2 = multiply(meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, %1) /* ty=Tensor[(16, 3, 3, 3), float32] */;\n",
      "  %3 = nn.conv2d(%data, %2, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %4 = multiply(meta[relay.Constant][2] /* ty=Tensor[(16), float32] */, %0) /* ty=Tensor[(16), float32] */;\n",
      "  %5 = nn.bias_add(%3, %4) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %6 = add(%5, meta[relay.Constant][3] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%6) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = squeeze(meta[relay.Constant][1] /* ty=Tensor[(16, 1, 1), float32] */, axis=[1, 2]) /* ty=Tensor[(16), float32] */;\n",
      "  %1 = expand_dims(%0, axis=1, num_newaxis=3) /* ty=Tensor[(16, 1, 1, 1), float32] */;\n",
      "  %2 = multiply(meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, %1) /* ty=Tensor[(16, 3, 3, 3), float32] */;\n",
      "  %3 = nn.conv2d(%data, %2, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %4 = multiply(meta[relay.Constant][2] /* ty=Tensor[(16), float32] */, %0) /* ty=Tensor[(16), float32] */;\n",
      "  %5 = nn.bias_add(%3, %4) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %6 = add(%5, meta[relay.Constant][3] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%6) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: FoldConstant, opt_level: 2, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = squeeze(meta[relay.Constant][1] /* ty=Tensor[(16, 1, 1), float32] */, axis=[1, 2]) /* ty=Tensor[(16), float32] */;\n",
      "  %1 = expand_dims(%0, axis=1, num_newaxis=3) /* ty=Tensor[(16, 1, 1, 1), float32] */;\n",
      "  %2 = multiply(meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, %1) /* ty=Tensor[(16, 3, 3, 3), float32] */;\n",
      "  %3 = nn.conv2d(%data, %2, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %4 = multiply(meta[relay.Constant][2] /* ty=Tensor[(16), float32] */, %0) /* ty=Tensor[(16), float32] */;\n",
      "  %5 = nn.bias_add(%3, %4) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %6 = add(%5, meta[relay.Constant][3] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%6) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0], padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %2 = add(%1, meta[relay.Constant][2] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%2) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %2 = add(%1, meta[relay.Constant][2] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%2) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: CanonicalizeOps, opt_level: 3, required passes: [\n",
      "InferType, ]\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %2 = add(%1, meta[relay.Constant][2] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%2) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %1 = expand_dims(meta[relay.Constant][1] /* ty=Tensor[(16), float32] */, axis=1, num_newaxis=2);\n",
      "  %2 = add(%0, %1) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %3 = add(%2, meta[relay.Constant][2] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%3) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: FoldConstant, opt_level: 2, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %1 = expand_dims(meta[relay.Constant][1] /* ty=Tensor[(16), float32] */, axis=1, num_newaxis=2) /* ty=Tensor[(16, 1, 1), float32] */;\n",
      "  %2 = add(%0, %1) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %3 = add(%2, meta[relay.Constant][2] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%3) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n",
      "运行 pass: The meta data of the pass - pass name: InferType, opt_level: 0, required passes: []\n",
      "\n",
      "fn (%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %1 = add(%0, meta[relay.Constant][1]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %2 = add(%1, meta[relay.Constant][2] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  nn.relu(%2) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */\n",
      "} /* ty=fn (Tensor[(1, 3, 4, 4), float32]) -> Tensor[(1, 16, 4, 4), float32] */\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@tvm.instrument.pass_instrument\n",
    "class PrintIR:\n",
    "    def run_before_pass(self, mod, info):\n",
    "        print(f\"运行 pass: {info}\")\n",
    "        print(mod[\"main\"])\n",
    "\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=3, instruments=[PrintIR()]):\n",
    "    with relay.quantize.qconfig(skip_conv_layers=[]):\n",
    "        qmod = relay.quantize.quantize(mod, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
