{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 归约运算\n",
    "\n",
    "归约（Reduction）是将输入张量的某个维度归约为标量的运算，例如 NumPy 的 `np.sum`。使用 for 循环通常可以直接实现归约。但是在 TVM 中有点复杂，因为不能直接使用 Python for 循环。在本节中，将描述如何在 TVM 中实现归约。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 1,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "# from tvm_book.contrib import d2ltvm\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 2
   },
   "source": [
    "## 求和\n",
    "\n",
    "从对二维矩阵的行求和开始把它变成一维向量。在 NumPy 中，可以用 `sum` 方法来完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "29"
    },
    "origin_pos": 3,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0295496, 3.784973 , 2.0665042], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.normal(size=(3,4)).astype('float32')\n",
    "a.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "和前面一样，从头实现这个运算，以帮助理解 TVM 表达式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "origin_pos": 5,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0295496, 3.784973 , 2.0665042], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_rows(a, b):\n",
    "    \"\"\"a is an n-by-m 2-D matrix, b is an n-length 1-D vector \n",
    "    \"\"\"\n",
    "    n = len(b)\n",
    "    for i in range(n):\n",
    "        b[i] = np.sum(a[i,:])\n",
    "\n",
    "b = np.empty((3,), dtype='float32')\n",
    "sum_rows(a, b)\n",
    "b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "这非常简单，首先对第一个维度进行迭代，`axis=0`，然后对第二个维度上的所有元素求和，以写入结果。在 NumPy 中，可以使用 `:` 对该维度上的所有元素进行切片。\n",
    "\n",
    "可以在 TVM 中实现同样的事情。与 [](ch_vector_add_te) 中的加法运算相比，这里使用了两个新的算子。一个是 `tvm.reduce_axis`，创建范围为 0 到 `m` 的归约轴。它的功能类似于 `sum_rows` 中使用的 `:`，但需要在 TVM 中显式指定范围。另一个是 `tvm.sum`，它对沿归约轴 `k` 的所有元素求和，并返回标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "30"
    },
    "origin_pos": 7,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrimFunc([a, b]) attrs={\"from_legacy_te_schedule\": (bool)1, \"global_symbol\": \"main\", \"tir.noalias\": (bool)1} {\n",
       "  for (i, 0, n) {\n",
       "    b[(i*stride)] = 0f\n",
       "    for (j, 0, m) {\n",
       "      b[(i*stride)] = (b[(i*stride)] + a[((i*stride) + (j*stride))])\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = te.var('n'), te.var('m')\n",
    "A = te.placeholder((n, m), name='a')\n",
    "j = te.reduce_axis((0, m), name='j')\n",
    "B = te.compute((n,), lambda i: te.sum(A[i, j], axis=j), name='b')\n",
    "s = te.create_schedule(B.op)\n",
    "ir_mod = tvm.lower(s, [A, B], simple_mode=True)\n",
    "ir_mod[\"main\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "可以看到，生成的伪代码将 `tvm.sum` 扩展成另一个沿着 `k` 轴的 for 循环。正如前面提到的，伪代码是类似于 C 语言的，因此将 `a[i,j]` 的索引展开为 `(i*m)+j`，将 `a` 处理为一维数组。还要注意，`b` 在求和之前被初始化为全零。\n",
    "\n",
    "现在测试结果与预期一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "origin_pos": 9,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "mod = tvm.build(ir_mod)\n",
    "c = tvm.nd.array(np.empty((3,), dtype='float32'))\n",
    "mod(tvm.nd.array(a), c)\n",
    "np.testing.assert_equal(b, c.asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "`a.sum()` 将对 `a` 中的所有元素求和，并返回标量。TVM 中实现它，需要沿着第一个维度的另一个轴归约，它的大小是 `n`。结果是标量，即 0 阶张量，可以用空元组 `()` 创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "31"
    },
    "origin_pos": 11,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrimFunc([a, b]) attrs={\"from_legacy_te_schedule\": (bool)1, \"global_symbol\": \"main\", \"tir.noalias\": (bool)1} {\n",
       "  b[0] = 0f\n",
       "  for (i, 0, n) {\n",
       "    for (j, 0, m) {\n",
       "      b[0] = (b[0] + a[((i*stride) + (j*stride))])\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = te.reduce_axis((0, n), name='i')\n",
    "B = te.compute((), lambda: te.sum(A[i, j], axis=(i, j)), name='b')\n",
    "s = te.create_schedule(B.op)\n",
    "ir_mod = tvm.lower(s, [A, B], simple_mode=True)\n",
    "ir_mod[\"main\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "验证结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    },
    "origin_pos": 13,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "mod = tvm.build(ir_mod)\n",
    "c = tvm.nd.array(np.empty((), dtype='float32'))\n",
    "mod(tvm.nd.array(a), c)\n",
    "np.testing.assert_allclose(a.sum(), c.asnumpy(), atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本例中，使用 `np.testing.assert_allclose` 而不是 `np.testing.assert_equal` 用于验证结果，因为 `float32` 数字的计算可能由于数值误差而不同。\n",
    "\n",
    "除 `tvm.sum` 之外，在 TVM 中还有其他的归约算子，如 `tvm.min`、`tvm.max`。也可以用它们来实现相应的归约运算。\n",
    "\n",
    "## 可交换的归约\n",
    "\n",
    "在数学中，如果 $a\\circ b = b\\circ a$，则算子 $\\circ$ 是可交换的（commutative）。TVM 允许通过 `tvm.comm_reducer` 定义定制的可交换归约算子。它接受两个函数参数，一个定义如何计算，另一个指定初始值。\n",
    "\n",
    "让我们以行为单位使用乘法，例如 `a.prod(axis=1)`。同样，首先展示如何从头开始实现它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "25"
    },
    "origin_pos": 15,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "def prod_rows(a, b):\n",
    "    \"\"\"a is an n-by-m 2-D matrix, b is an n-length 1-D vector \n",
    "    \"\"\"\n",
    "    n, m = a.shape\n",
    "    for i in range(n):\n",
    "        b[i] = 1\n",
    "        for j in range(m):\n",
    "            b[i] = b[i] * a[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，首先需要将返回值初始化为 1，然后使用标量积 `*` 计算归约。现在让我们在 TVM 中定义这两个函数，作为 `te.comm_reducer` 的参数。如前所述，第一个函数定义两个标量输入的 $a\\circ b$。第二个参数接受数据类型参数，以返回元素的初始值。然后可以创建归约算子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "comp = lambda a, b: a * b\n",
    "init = lambda dtype: tvm.tir.const(1, dtype=dtype)\n",
    "product = te.comm_reducer(comp, init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "`product` 的用法类似于 `te.sum`。实际上，`te.sum` 是预定义的归约算子，使用相同的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "26"
    },
    "origin_pos": 19,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrimFunc([a, b]) attrs={\"from_legacy_te_schedule\": (bool)1, \"global_symbol\": \"main\", \"tir.noalias\": (bool)1} {\n",
       "  for (i, 0, n) {\n",
       "    b[(i*stride)] = 1f\n",
       "    for (k, 0, m) {\n",
       "      b[(i*stride)] = (b[(i*stride)]*a[((i*stride) + (k*stride))])\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = te.var('n')\n",
    "m = te.var('m')\n",
    "A = te.placeholder((n, m), name='a')\n",
    "k = te.reduce_axis((0, m), name='k')\n",
    "B = te.compute((n,), lambda i: product(A[i, k], axis=k), name='b')\n",
    "s = te.create_schedule(B.op)\n",
    "ir_mod = tvm.lower(s, [A, B], simple_mode=True)\n",
    "ir_mod[\"main\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "生成的伪代码与按行求和的伪代码类似，只是初始值和归约算法不同。\n",
    "\n",
    "再来验证一下结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "28"
    },
    "origin_pos": 21,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "mod = tvm.build(ir_mod)\n",
    "b = tvm.nd.array(np.empty((3,), dtype='float32'))\n",
    "mod(tvm.nd.array(a), b)\n",
    "np.testing.assert_allclose(a.prod(axis=1), b.asnumpy(), atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "## 小结\n",
    "\n",
    "- 可以应用归约算子，例如 `te.sum` 对归约轴 `te.reduce_axis` 求和。\n",
    "- 可以通过 `te.comm_reducer` 实现定制的可交换归约算子。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f0a0fcc4cb7375f8ee907b3c51d5b9d65107fda1aab037a85df7b0c09b870b98"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
