{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "(ch_batch_norm)=\n",
    "# Batch Normalization\n",
    "\n",
    "本节讨论如何使用 TVM 执行 batch normalization（`batch_norm`）。和 pooling 一样，`batch_norm` 也是 CNN 中常见的算子。D2L 在 [细节](https://d2l.ai/chapter_convolutional-modern/batch-norm.html) 中引入了这个算子。\n",
    "\n",
    "从计算的角度来看，对于给定的值，`batch_norm` 减去其中的 `mean`，然后用 `variance` 的平方根除以它，与常规 normalization 没有区别。它被称为 `batch_norm`，因为均值和方差是在执行训练时从批次中获得的。在此之后，  `batch_norm` 也对该值应用仿射变换，即将其与 scale 值 $\\gamma$ 相乘，然后加上 shift 值 $\\beta$。由训练梯度计算得到 $\\gamma$ 和 $\\beta$。最后，加上一个小的正值 $\\epsilon$，以防止除数为 0。\n",
    "\n",
    "在推理的情况下，均值和方差都是确定的，因此 `batch_norm` 的过程只是几个简单的元素运算的组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "origin_pos": 1,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import te\n",
    "from tvm_book.contrib import d2ltvm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 2
   },
   "source": [
    "## 计算定义\n",
    "\n",
    "在实践中，不打算执行一个值的 `batch_norm`。相反，`batch_norm` 将在卷积的输出上执行，即 (channel, height, weight) 中的 3-D 数据。\n",
    "\n",
    "$$out[i,:,:] = \\frac{data[i,:,:] - mean[i]}{\\sqrt{var[i]+\\epsilon}} \\\n",
    "* \\gamma[i] + \\beta[i] $$\n",
    "\n",
    "在模型训练过程中，并根据输入 $data$ 进行计算 $mean$ 和 $var$。然而，这里重点关注模型推理，并给出了 $mean$ 和 $var$；因此不需要从中计算它们。\n",
    "\n",
    "我们将定义这个公式的计算。实际上，`batch_norm` 是一些简单的广播和元素相关的计算的组合。注意，在 {ref}`ch_bcast_add` 中，定义了有限的 `broadcast_add`，只对二维张量执行广播加法。如果将其推广到更多维和更多的 calculator，就可以重用它们来组成 `batch_norm` 算子。这就是 TVM 所做的。\n",
    "\n",
    "这里，为了简单起见，使用 TVM 基本算子进行广播计算。TVM 算子定义在 `TOPI` 中，TOPI 代表张量算子清单。它遵循 NumPy 约定来覆盖算术算子（即 `+`、`-`、`*`、`/`）。元素的平方根也可以在 `TOPI` 中找到。\n",
    "\n",
    "定义 `batch_norm` 的代码片段如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "origin_pos": 3,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "# Save to the d2ltvm package.\n",
    "from tvm_book.contrib.d2ltvm import topi\n",
    "\n",
    "def batch_norm(c, n, eps=1e-5):\n",
    "    \"\"\"batch normalization\n",
    "    \n",
    "    c : channels\n",
    "    N : input width and height\n",
    "    eps : small positive value to prevent divide 0\n",
    "    \"\"\"\n",
    "        \n",
    "    X = te.placeholder((c, n, n), name='X')\n",
    "    Mean = te.placeholder((c, 1, 1), name='Mean')\n",
    "    Var = te.placeholder((c, 1, 1), name='Var')\n",
    "    Gamma = te.placeholder((c, 1, 1), name='Gamma')\n",
    "    Beta = te.placeholder((c, 1, 1), name='Beta')\n",
    "    C1 = X - Mean\n",
    "    C2 = topi.sqrt(Var + eps)\n",
    "    Y = C1 / C2 * Gamma + Beta\n",
    "    return X, Mean, Var, Gamma, Beta, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "然后可以打印并编译 IR。IR 包括几个阶段，但应该很容易遵循。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 5,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrimFunc([X, Mean, Var, Gamma, Beta, T_add]) attrs={\"from_legacy_te_schedule\": (bool)1, \"global_symbol\": \"main\", \"tir.noalias\": (bool)1} {\n",
       "  allocate T_subtract[float32 * 25088], storage_scope = global\n",
       "  allocate T_add[float32 * 32], storage_scope = global\n",
       "  for (ax0, 0, 32) {\n",
       "    for (ax1, 0, 28) {\n",
       "      for (ax2, 0, 28) {\n",
       "        let cse_var_1 = (((ax0*784) + (ax1*28)) + ax2)\n",
       "        T_subtract[cse_var_1] = (X[cse_var_1] - Mean[ax0])\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  for (ax0, 0, 32) {\n",
       "    T_add[ax0] = (Var[ax0] + 1e-05f)\n",
       "  }\n",
       "  for (i0, 0, 32) {\n",
       "    T_add[i0] = tir.sqrt(T_add[i0])\n",
       "  }\n",
       "  for (ax0, 0, 32) {\n",
       "    for (ax1, 0, 28) {\n",
       "      for (ax2, 0, 28) {\n",
       "        let cse_var_2 = (((ax0*784) + (ax1*28)) + ax2)\n",
       "        T_subtract[cse_var_2] = (T_subtract[cse_var_2]/T_add[ax0])\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  for (ax0, 0, 32) {\n",
       "    for (ax1, 0, 28) {\n",
       "      for (ax2, 0, 28) {\n",
       "        let cse_var_3 = (((ax0*784) + (ax1*28)) + ax2)\n",
       "        T_subtract[cse_var_3] = (T_subtract[cse_var_3]*Gamma[ax0])\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  for (ax0, 0, 32) {\n",
       "    for (ax1, 0, 28) {\n",
       "      for (ax2, 0, 28) {\n",
       "        let cse_var_4 = (((ax0*784) + (ax1*28)) + ax2)\n",
       "        T_add[cse_var_4] = (T_subtract[cse_var_4] + Beta[ax0])\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 32\n",
    "n = 28\n",
    "X, Mean, Var, Gamma, Beta, Y = batch_norm(c, n)\n",
    "\n",
    "sch = te.create_schedule(Y.op)\n",
    "m = tvm.lower(sch, [X, Mean, Var, Gamma, Beta, Y], simple_mode=True)\n",
    "mod = tvm.build(m)\n",
    "\n",
    "m[\"main\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "要执行它，需要为 `batch_norm` 创建数据。与前面获取 conv 和 pooling 数据的部分类似，定义了 `get_bn_data` 方法来生成 `batch_norm` 的数据。棘手的问题是方差必须是非负数。因此，将随机数生成器正态分布的均值移至 1（默认均值为 0，标准差为 1），得到生成结果的绝对数量。\n",
    "\n",
    "在获得数据之后，可以简单地调用编译后的模块来执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "origin_pos": 7,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "# Save to the d2ltvm package.\n",
    "def get_bn_data(c, n, constructor=None):\n",
    "    \"\"\"Return the batch norm data, mean, variance, gamma and beta tensors.\n",
    "       Also return the empty tensor for output.\n",
    "\n",
    "    c : channels\n",
    "    n : input width and height\n",
    "    constructor : user-defined tensor constructor\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    data = np.random.normal(size=(c, n, n)).astype('float32')\n",
    "    mean = np.random.normal(size=(c, 1, 1)).astype('float32')\n",
    "    # move the mean of the normal distribution to be 1\n",
    "    var = np.random.normal(loc=1.0, size=(c, 1, 1)).astype('float32')\n",
    "    # make sure all variance numbers are not negative\n",
    "    var = np.absolute(var)\n",
    "    gamma = np.random.normal(size=(c, 1, 1)).astype('float32')\n",
    "    beta = np.random.normal(size=(c, 1, 1)).astype('float32')\n",
    "    out = np.empty((c, n, n), dtype='float32')\n",
    "    if constructor:\n",
    "        data, mean, var, gamma, beta, out = \\\n",
    "        (constructor(x) for x in [data, mean, var, gamma, beta, out])\n",
    "    return data, mean, var, gamma, beta, out\n",
    "\n",
    "data, mean, var, gamma, beta, out = get_bn_data(c, n, tvm.nd.array)\n",
    "mod(data, mean, var, gamma, beta, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "## MXNet Baseline\n",
    "\n",
    "使用 MXNet 的 `batch_norm` 函数作为基准来检查编译函数的正确性。\n",
    "\n",
    "MXNet 中的这个函数被定义为通用函数，用于训练和推理。在这里讨论的推理案例中，需要正确地设置相应的输入参数。一个是 `use_global_stats`，需要设置为 `True` ，因为将使用 `batch_norm` 的输入平均值和方差来计算，而不是从输入数据计算它们（训练将这样做）。另一个是 `fix_gamma`，它需要设置为 `False`，这样输入的 $\\gamma$ 就会被使用，而不是将 $\\gamma$ 为全设置为 1。\n",
    "\n",
    "最后，就像在其他例子中讨论的那样，MXNet `batch_norm` 的输入数据是 4D，其中 batch 是最外维。因此，将在数据中相应地扩展这个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "origin_pos": 9,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "# Save to the d2ltvm package.\n",
    "def get_bn_data_mxnet(c, n, ctx='cpu'):\n",
    "    ctx = getattr(mx, ctx)()\n",
    "    data, mean, var, gamma, beta, out = get_bn_data(c, n,\n",
    "                                      lambda x: mx.nd.array(x, ctx=ctx))\n",
    "    data, out = data.expand_dims(axis=0), out.expand_dims(axis=0)\n",
    "    return data, mean, var, gamma, beta, out\n",
    "\n",
    "# Save to the d2ltvm package.\n",
    "def batch_norm_mxnet(data, mean, var, gamma, beta, out, eps=1e-5):\n",
    "    # use_global_stats=True to use the input mean and var instead of computing\n",
    "    # the mean and var of the input data.\n",
    "    # fix_gamma=False so that gamma won't be set to 1.\n",
    "    mx.nd.BatchNorm(data, gamma, beta, mean, var, eps, \n",
    "                    use_global_stats=True, fix_gamma=False, out=out)\n",
    "\n",
    "data, mean, var, gamma, beta, out_mx = get_bn_data_mxnet(c, n)\n",
    "batch_norm_mxnet(data, mean, var, gamma, beta, out_mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "最后，检查结果是否与 MXNet 产生的结果足够接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "origin_pos": 11,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(out_mx[0].asnumpy(), out.asnumpy(), atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "## 小结\n",
    "\n",
    "- 从计算的角度来看，`batch_norm` 是一系列广播和元素算子的组合，可以很容易地从 TVM 的张量算子库（Tensor OPerator Inventory，简称 TOPI）中得到。\n",
    "- 在推理中，`batch_norm` 的 $mean$ 和 $var$ 是预定义的。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f0a0fcc4cb7375f8ee907b3c51d5b9d65107fda1aab037a85df7b0c09b870b98"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
