{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "(ch_pooling)=\n",
    "# 池化\n",
    "\n",
    "本节讨论如何使用 TVM 进行池。Pooling 是 CNN 中常见的算子，如果你不熟悉它，请参考 D2L 的 [6.5](https://d2l.ai/chapter_convolutional-neural-networks/pooling.html) 章节。在这里将跳过为什么，只关注如何。\n",
    "\n",
    "池有两种类型，`max pooling` 返回池的最大值，`avg pooling` 返回池的平均值。为了简单起见，在本节中处理 2D 池化。与 conv2d 一样，池化算子在特征图中移动了池化核。有时需要填充以匹配所需的输出大小。Pooling 比 conv2d 的计算量要少得多，因为它只需要得到最大值或平均值。它是 memory-bound 算子。\n",
    "\n",
    "{ref}`fig_pooling` 说明了 2D max pooling 和平均 pooling 是如何工作的，使用以下设置：kernel 尺寸 [3,3]， stride [1,1]， padding[1,1]。\n",
    "\n",
    "(fig_pooling)=\n",
    "```{figure} ../img/pooling.svg\n",
    "2D 最大值和平均池化。蓝色形状表示特定的池化步骤。注意，除了算法之外，填充值也不同。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 1,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import te\n",
    "from tvm_book.contrib import d2ltvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 2
   },
   "source": [
    "## 定义计算\n",
    "\n",
    "`pooling` 的计算方式类似于 `conv`，所以你会发现下面的池化定义代码接受与 {ref}`ch_conv` 中定义的 `conv` 相似的参数。池化的输出大小也可以通过重用 `conv_out_size` 方法来计算。\n",
    "\n",
    "使用不同的 `te.compute` 在同一方法中包含两种类型的 `pooling`。如果未指定 `pool_type`，则该方法将抛出错误。使用 `te.max` 来执行 `max pooling` 和 `te.sum` 和元素除法来执行 `avg pooling`。此外，还请注意 `max pooling` 的填充值是 `te.min_value`，而 `avg pooling` 为 0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "# Save to the d2ltvm package.\n",
    "def pool(pool_type, c, nh, nw, kh, kw, ph=0, pw=0, sh=1, sw=1):\n",
    "    \"\"\"2D pooling\n",
    "    \n",
    "    pool_type: pooling type, 'max' or 'avg'\n",
    "    c : channels\n",
    "    nh, nw : input width and height\n",
    "    kh, kw : kernel width and height\n",
    "    ph, pw : height and width padding sizes, default 0\n",
    "    sh, sw : height and width strides, default 1\n",
    "    \"\"\"\n",
    "    # reduction axes\n",
    "    rkh = te.reduce_axis((0, kh), name='rkh')\n",
    "    rkw = te.reduce_axis((0, kw), name='rkw')\n",
    "    # output height and weights\n",
    "    oh = d2ltvm.conv_out_size(nh, kh, ph, sh)\n",
    "    ow = d2ltvm.conv_out_size(nw, kw, pw, sw)\n",
    "    # pad X and then compute Y\n",
    "    X = te.placeholder((c, nh, nw), name='X')\n",
    "    \n",
    "    \n",
    "    if pool_type == 'max':\n",
    "        PaddedX = d2ltvm.padding(X, ph, pw, val=te.min_value(X.dtype)) \\\n",
    "            if ph * pw != 0 else X\n",
    "        Y = te.compute((c, oh, ow), \\\n",
    "                            lambda c, h, w: \\\n",
    "                            te.max(PaddedX[c, h*sh+rkh, w*sw+rkw], \\\n",
    "                                axis=[rkh, rkw]), \\\n",
    "                            tag=\"pool_max\", name='PoolMax')\n",
    "    elif pool_type == 'avg':\n",
    "        PaddedX = d2ltvm.padding(X, ph, pw) if ph * pw != 0 else X\n",
    "        tsum = te.compute((c, oh, ow), \\\n",
    "                            lambda c, h, w: \\\n",
    "                            te.sum(PaddedX[c, h*sh+rkh, w*sw+rkw], \\\n",
    "                                axis=[rkh, rkw]), \\\n",
    "                            tag=\"pool_avg1\", name='PoolSum')\n",
    "        Y = te.compute((c, oh, ow), \\\n",
    "                            lambda c, h, w: \\\n",
    "                            tsum[c, h, w] / (kh*kw), \\\n",
    "                            tag='pool_avg2', name='PoolAvg')\n",
    "    else:\n",
    "        raise ValueError(\"Pool type should be 'avg' or 'max'.\")\n",
    "    return X, Y, PaddedX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "然后，使用一些简单的数据大小来编译 `max pooling`。计算逻辑很简单，如 IR 所示。同样，{ref}`ch_conv` 中的 `get_conv_data` 方法可以重用来初始化数据。注意，在本例中不需要权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 5,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrimFunc([X, PoolMax]) attrs={\"from_legacy_te_schedule\": (bool)1, \"global_symbol\": \"main\", \"tir.noalias\": (bool)1} {\n",
       "  allocate PaddedX[float32 * 784], storage_scope = global\n",
       "  for (i0, 0, 4) {\n",
       "    for (i1, 0, 14) {\n",
       "      for (i2, 0, 14) {\n",
       "        PaddedX[(((i0*196) + (i1*14)) + i2)] = tir.if_then_else(((((i1 < 1) || (13 <= i1)) || (i2 < 1)) || (13 <= i2)), -3.40282e+38f, X[((((i0*144) + (i1*12)) + i2) - 13)])\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  for (c, 0, 4) {\n",
       "    for (h, 0, 12) {\n",
       "      for (w, 0, 12) {\n",
       "        PoolMax[(((c*144) + (h*12)) + w)] = -3.40282e+38f\n",
       "        for (rkh, 0, 3) {\n",
       "          for (rkw, 0, 3) {\n",
       "            let cse_var_1 = (((c*144) + (h*12)) + w)\n",
       "            PoolMax[cse_var_1] = max(PoolMax[cse_var_1], PaddedX[(((((c*196) + (h*14)) + (rkh*14)) + w) + rkw)])\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c, n, k, p, s = 4, 12, 3, 1, 1\n",
    "X, Y, PaddedX = pool('max', c, n, n, k, k, p, p, s, s)\n",
    "sch = te.create_schedule(Y.op)\n",
    "\n",
    "m = tvm.lower(sch, [X, Y], simple_mode=True)\n",
    "data, _, out_max = d2ltvm.get_conv_data(c, c, n, k, p, s, tvm.nd.array)\n",
    "\n",
    "mod = tvm.build(m)\n",
    "mod(data, out_max)\n",
    "m[\"main\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "接下来，使用相同的玩具数据大小编译 `avg pooling`。计算逻辑也很简单。检查计算以及从 `max pooling` 填充值的差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrimFunc([X, PoolAvg]) attrs={\"from_legacy_te_schedule\": (bool)1, \"global_symbol\": \"main\", \"tir.noalias\": (bool)1} {\n",
       "  allocate PaddedX[float32 * 784], storage_scope = global\n",
       "  allocate PoolSum[float32 * 576], storage_scope = global\n",
       "  for (i0, 0, 4) {\n",
       "    for (i1, 0, 14) {\n",
       "      for (i2, 0, 14) {\n",
       "        PaddedX[(((i0*196) + (i1*14)) + i2)] = tir.if_then_else(((((i1 < 1) || (13 <= i1)) || (i2 < 1)) || (13 <= i2)), 0f, X[((((i0*144) + (i1*12)) + i2) - 13)])\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  for (c, 0, 4) {\n",
       "    for (h, 0, 12) {\n",
       "      for (w, 0, 12) {\n",
       "        PoolSum[(((c*144) + (h*12)) + w)] = 0f\n",
       "        for (rkh, 0, 3) {\n",
       "          for (rkw, 0, 3) {\n",
       "            let cse_var_1 = (((c*144) + (h*12)) + w)\n",
       "            PoolSum[cse_var_1] = (PoolSum[cse_var_1] + PaddedX[(((((c*196) + (h*14)) + (rkh*14)) + w) + rkw)])\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  for (c, 0, 4) {\n",
       "    for (h, 0, 12) {\n",
       "      for (w, 0, 12) {\n",
       "        let cse_var_2 = (((c*144) + (h*12)) + w)\n",
       "        PoolAvg[cse_var_2] = (PoolSum[cse_var_2]*0.111111f)\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y, PaddedX = pool('avg', c, n, n, k, k, p, p, s, s)\n",
    "sch = te.create_schedule(Y.op)\n",
    "\n",
    "m = tvm.lower(sch, [X, Y], simple_mode=True)\n",
    "data, _, out_avg = d2ltvm.get_conv_data(c, c, n, k, p, s, tvm.nd.array)\n",
    "mod = tvm.build(m)\n",
    "mod(data, out_avg)\n",
    "m[\"main\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "## MXNet Baseline\n",
    "\n",
    "使用 MXNet 的 pooling 函数作为基线来检查编译函数的正确性。MXNet 计算池的方式与我们所做的类似。唯一不同的是它的输入数据是 4D，其中最外维是 batch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "# Save to the d2ltvm package.\n",
    "def get_pool_data_mxnet(c, n, k, p, s, ctx='cpu'):\n",
    "    ctx = getattr(mx, ctx)()\n",
    "    data, _, out = d2ltvm.get_conv_data(c, c, n, k, p, s,\n",
    "                                      lambda x: mx.nd.array(x, ctx=ctx))\n",
    "    data, out = data.expand_dims(axis=0), out.expand_dims(axis=0)\n",
    "    return data, out\n",
    "\n",
    "# Save to the d2ltvm package.\n",
    "def pool_mxnet(pool_type, data, out, k, p, s):\n",
    "    mx.nd.Pooling(data, kernel=(k,k), stride=(s,s),\n",
    "                      pad=(p,p), pool_type=pool_type, out=out)\n",
    "\n",
    "data, out_max_mx = get_pool_data_mxnet(c, n, k, p, s)\n",
    "pool_mxnet('max', data, out_max_mx, k, p, s)\n",
    "data, out_avg_mx = get_pool_data_mxnet(c, n, k, p, s)\n",
    "pool_mxnet('avg', data, out_avg_mx, k, p, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "最后，检查结果是否与 MXNet 产生的结果足够接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.testing.assert_allclose(out_max_mx[0].asnumpy(), out_max.asnumpy(), atol=1e-5)\n",
    "np.testing.assert_allclose(out_avg_mx[0].asnumpy(), out_avg.asnumpy(), atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "## 小结\n",
    "\n",
    "- 2D 池化处理数据的方式与 2D 卷积类似，但计算本身更轻量。\n",
    "- 可以使用 TVM 表达式轻松定义 `max pooling` 和 `avg pooling`。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f0a0fcc4cb7375f8ee907b3c51d5b9d65107fda1aab037a85df7b0c09b870b98"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
