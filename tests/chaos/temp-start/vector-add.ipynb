{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "(ch_vector_add_te)=\n",
    "# 向量加法(te)\n",
    "\n",
    "编写程序：对两个 `n` 维向量 `a` 和 `b` 求和。这在 NumPy 中很简单，可以用 `c = a + b` 来做。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "origin_pos": 1,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_abc(shape, constructor=None):\n",
    "    \"\"\"返回随机变量 a, b 和空变量 c\n",
    "\n",
    "    参数\n",
    "    ====\n",
    "    shape: 数据形状\n",
    "    constructor: 数据变换\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    a = np.random.normal(size=shape).astype(np.float32)\n",
    "    b = np.random.normal(size=shape).astype(np.float32)\n",
    "    c = np.empty_like(a)\n",
    "    if constructor:\n",
    "        a, b, c = [constructor(x) for x in (a, b, c)]\n",
    "    return a, b, c"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建两个长度为 100 的随机向量，并对它们进行元素求和。注意，NumPy 在默认情况下使用 64 位浮点数或 64 位整数，这与深度学习中通常使用的 32 位浮点数不同，因此需要显式转换数据类型。\n",
    "\n",
    "虽然可以在 NumPy 中使用内置的 `+` 算子（operator）来实现元素级的加法，但这里尝试仅使用标量算子来实现它。它将帮助理解 TVM 的实现。下面的函数使用 `for` 循环迭代向量中的每个元素，然后每次使用标量（scalar） `+` 算子将两个元素相加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    },
    "origin_pos": 3,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "def vector_add(a, b, c):\n",
    "    n = len(a)\n",
    "    for i in range(n):\n",
    "        c[i] = a[i] + b[i]\n",
    "\n",
    "n = 100\n",
    "a, b, c = get_abc((n,))\n",
    "d = a + b # 参考结果\n",
    "vector_add(a, b, c)\n",
    "np.testing.assert_array_equal(c, d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## 张量表达式(te)\n",
    "\n",
    "现在开始在 TVM 中实现 {func}`vector_add`。TVM 的实现有两点不同：\n",
    "\n",
    "1. 不需要编写完整的函数，只需要指定每个元素的输出，即 `c[i]`\n",
    "2. TVM 是符号化的，通过指定形状来创建符号变量，并定义程序如何计算\n",
    "\n",
    "在下面的程序中，首先通过 {func}`~tvm.te.operation.placeholder` 来指定两个输入的占位符 `A` 和 `B` 的形状 `(n,)`。`A` 和 `B` 都是 `Tensor` 对象，可以稍后提供数据。给它们赋值名称，这样以后就可以打印易于阅读的程序。\n",
    "\n",
    "接下来，定义如何通过 {func}`~tvm.te.operation.compute` 计算输出 `C`。它接受两个参数：输出形状和通过给定索引来计算每个元素的函数。由于输出是向量，它的元素按整数索引。在 {func}`~tvm.te.operation.compute` 中定义的 lambda 函数接受单个参数 `i`，并返回 `c[i]`，它与 {func}`vector_add` 中定义的 `c[i] = a[i] + b[i]` 相同。不同之处在于，不用编写 for 循环，它将在稍后由 TVM 填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "26"
    },
    "origin_pos": 7,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tvm.te.tensor.Tensor, tvm.te.tensor.Tensor)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm import te  # te 代表张量表达式（tensor expression）\n",
    "\n",
    "def vector_add(n):\n",
    "    \"\"\"TVM expression for vector add\"\"\"\n",
    "    A = te.placeholder((n,), name='a')\n",
    "    B = te.placeholder((n,), name='b')\n",
    "    C = te.compute(A.shape, lambda i: A[i] + B[i], name='c')\n",
    "    return A, B, C\n",
    "\n",
    "\n",
    "A, B, C = vector_add(n)\n",
    "type(A), type(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到 `A`、`B` 和 `C` 都是 {class}`~tvm.te.tensor.Tensor` 对象，它可以被视为 NumPy ndarray 的符号版本。可以访问变量的属性，比如数据类型和形状。但这些值目前还没有具体值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('float32', [100]), ('float32', [100]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A.dtype, A.shape), (C.dtype, C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "32"
    },
    "origin_pos": 9,
    "tab": [
     "tvm"
    ]
   },
   "source": [
    "生成张量对象的算子可以被 `.op` 访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "54"
    },
    "origin_pos": 11,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tvm.te.tensor.PlaceholderOp, tvm.te.tensor.ComputeOp)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(A.op), type(C.op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到 `A` 和 `C` 的算子类型是不同的，但它们共享同一个基类 {class}`~tvm.te.tensor.Operation`，这个基类表示生成张量对象的算子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "44"
    },
    "origin_pos": 13,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tvm.te.tensor.Operation, tvm.te.tensor.BaseComputeOp)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.op.__class__.__bases__[0], C.op.__class__.__bases__[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issubclass(tvm.te.tensor.BaseComputeOp, tvm.te.tensor.Operation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## 创建 te Schedule\n",
    "\n",
    "为了运行计算，需要指定如何执行程序，例如，访问数据的顺序以及如何进行多线程并行化。\n",
    "这样的执行计划被称为 **调度** （schedule）。`C` 是输出张量，可以在它的算子上创建默认调度，并打印伪代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "48"
    },
    "origin_pos": 15,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schedule(0x2a949b0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = te.create_schedule(C.op)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调度由几个阶段（Stage）组成。每个阶段对应于描述它是如何调度的算子。可以通过 `s[C]` 或 `s[C.op]` 进入特定的阶段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tvm.te.schedule.Schedule, tvm.te.schedule.Stage)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s), type(s[C])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "稍后将看到如何更改执行计划，以便更好地利用硬件资源来提高其效率。下面通过打印 C-like 的伪代码来查看默认的执行计划。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 19,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import ir as I\n",
      "# from tvm.script import tir as T\n",
      "\n",
      "@I.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def main(a: T.Buffer((100,), \"float32\"), b: T.Buffer((100,), \"float32\"), c: T.Buffer((100,), \"float32\")):\n",
      "        T.func_attr({\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True})\n",
      "        for i in range(100):\n",
      "            c_1 = T.Buffer((100,), data=c.data)\n",
      "            a_1 = T.Buffer((100,), data=a.data)\n",
      "            b_1 = T.Buffer((100,), data=b.data)\n",
      "            c_1[i] = a_1[i] + b_1[i]\n"
     ]
    }
   ],
   "source": [
    "m = tvm.lower(s, [A, B, C], simple_mode=True)\n",
    "print(m)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{func}`tvm.lower` 方法接受 schedule、输入、输出张量。`simple_mode=True` 将以简单而紧凑的方式打印程序。注意，程序已经根据输出形状添加了适当的 for 循环。总的来说，它非常类似于前面的函数 `vector_add`。\n",
    "\n",
    "可以看到 TVM 将计算和调度分开。计算定义了如何计算结果，无论在什么硬件平台上运行程序，结果都不会改变。另一方面，有效的调度通常依赖于硬件，但是更改调度不会影响其正确性。TVM 从 Halide {cite:p}`Ragan-Kelley.Barnes.Adams.ea.2013` 那里继承了计算与调度分离的想法。\n",
    "\n",
    "## 计算并执行 ADD\n",
    "\n",
    "一旦定义了计算和调度，就可以使用 {func}`tvm.build` 将它们编译成可执行模块。它接受与 {func}`tvm.lower` 相同的参数。实际上，它首先调用了 {func}`tvm.lower` 生成中间表示程序，然后编译成机器码。即\n",
    "\n",
    "```python\n",
    "mod = tvm.build(s, [A, B, C])\n",
    "mod\n",
    "```\n",
    "\n",
    "或者，直接把中间表示模块编译为结果模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    },
    "origin_pos": 21,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.driver.build_module.OperatorModule"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = tvm.build(m)\n",
    "type(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它返回可执行模块对象。可以输入 `A`，`B` 和 `C` 的数据来运行它。张量数据必须是 {class}`tvm.runtime.ndarray.NDArray` 对象。最简单的方法是首先创建 NumPy ndarray 对象，然后通过 {func}`tvm.nd.array` 将它们转换为 TVM ndarray 对象。可以通过 {meth}`.numpy` 方法将它们转换回 NumPy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "origin_pos": 23,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tvm.runtime.ndarray.NDArray, array([1., 1.]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones(2)\n",
    "y = tvm.nd.array(x)\n",
    "type(y), y.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "构造数据并将它们作为 TVM ndarray 返回："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "origin_pos": 25,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "a_nd, b_nd, c_nd = [tvm.nd.array(k) for k in [a, b, c]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "进行计算，并验证结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "origin_pos": 27,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "mod(a_nd, b_nd, c_nd)\n",
    "np.testing.assert_array_equal(a_nd.numpy() + b_nd.numpy(), c_nd.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "## 参数约束\n",
    "\n",
    "记住，当声明 `A` 和 `B` 时，将两个输入都指定长度为 100 的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    },
    "origin_pos": 29,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([100], [100], [100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, B.shape, C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "TVM 将检查输入形状是否满足此规格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    },
    "origin_pos": 31,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  1: TVMFuncCall\n",
      "  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n",
      "  File \"/media/pc/data/lxw/ai/tvm/src/runtime/library_module.cc\", line 80\n",
      "TVMError: \n",
      "---------------------------------------------------------------\n",
      "An error occurred during the execution of TVM.\n",
      "For more information, please see: https://tvm.apache.org/docs/errors.html\n",
      "---------------------------------------------------------------\n",
      "\n",
      "  Check failed: ret == 0 (-1 vs. 0) : Assert fail: T.Cast(\"int32\", arg_a_shape[0]) == 100, Argument arg.a.shape[0] has an unsatisfied constraint: 100 == T.Cast(\"int32\", arg_a_shape[0])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    a, b, c = get_abc(200, tvm.nd.array)\n",
    "    mod(a, b, c)\n",
    "except tvm.TVMError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "TVM 默认数据类型为 `float32`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    },
    "origin_pos": 33,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('float32', 'float32', 'float32')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.dtype, B.dtype, C.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "如果输入的数据类型不同，则会出现错误。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    },
    "origin_pos": 35,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  1: TVMFuncCall\n",
      "  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n",
      "  File \"/media/pc/data/lxw/ai/tvm/src/runtime/library_module.cc\", line 80\n",
      "TVMError: \n",
      "---------------------------------------------------------------\n",
      "An error occurred during the execution of TVM.\n",
      "For more information, please see: https://tvm.apache.org/docs/errors.html\n",
      "---------------------------------------------------------------\n",
      "\n",
      "  Check failed: ret == 0 (-1 vs. 0) : Assert fail: T.tvm_struct_get(arg_a, 0, 5, \"uint8\") == T.uint8(2) and T.tvm_struct_get(arg_a, 0, 6, \"uint8\") == T.uint8(32) and T.tvm_struct_get(arg_a, 0, 7, \"uint16\") == T.uint16(1), arg.a.dtype is expected to be float32\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    a, b, c = get_abc(100, tvm.nd.array)\n",
    "    a = tvm.nd.array(a.numpy().astype('float64'))\n",
    "    mod(a, b, c)\n",
    "except tvm.TVMError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "## 保存和加载模块\n",
    "\n",
    "编译好的模块可以保存到磁盘中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    },
    "origin_pos": 37,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "mod_fname = 'vector-add.tar'\n",
    "mod.export_library(mod_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 38
   },
   "source": [
    "然后再加载回来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    },
    "origin_pos": 39,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "loaded_mod = tvm.runtime.load_module(mod_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "验证结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    },
    "origin_pos": 41,
    "tab": [
     "tvm"
    ]
   },
   "outputs": [],
   "source": [
    "a, b, c = get_abc(100, tvm.nd.array)\n",
    "loaded_mod(a, b, c)\n",
    "np.testing.assert_array_equal(a.numpy() + b.numpy(), c.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 42
   },
   "source": [
    "## 小结\n",
    "\n",
    "使用 TVM 实现 `te` 算子有三个步骤：\n",
    "\n",
    "1. 通过指定输入和输出形状以及如何计算每个输出元素来声明计算。\n",
    "2. 创建 `te` 调度，（希望）充分利用机器资源。\n",
    "3. 编译到硬件目标。\n",
    "\n",
    "此外，可以将编译后的模块保存到磁盘中，以便稍后再将其加载回来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "34e95b0948f576614c7863cc780d83f61f9551597d4ec05ab5fbb4cfe73deb20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
