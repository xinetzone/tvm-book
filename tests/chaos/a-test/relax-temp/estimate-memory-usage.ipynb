{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `estimate_memory_usage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "import tvm.testing\n",
    "from tvm.script import relax as R, tir as T\n",
    "from tvm.relax.analysis import estimate_memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class Module:\n",
    "    @T.prim_func\n",
    "    def add(\n",
    "        rxplaceholder: T.Buffer(T.int64(8), \"float32\"),\n",
    "        rxplaceholder_1: T.Buffer((), \"float32\"),\n",
    "        T_add: T.Buffer(T.int64(8), \"float32\"),\n",
    "    ):\n",
    "        T.evaluate(0)\n",
    "\n",
    "    @T.prim_func\n",
    "    def reshape(\n",
    "        rxplaceholder: T.Buffer((T.int64(2), T.int64(4)), \"float32\"),\n",
    "        T_reshape: T.Buffer(T.int64(8), \"float32\"),\n",
    "    ):\n",
    "        T.evaluate(0)\n",
    "\n",
    "    @T.prim_func\n",
    "    def relu(\n",
    "        rxplaceholder: T.Buffer(T.int64(8), \"float32\"), compute: T.Buffer(T.int64(8), \"float32\")\n",
    "    ):\n",
    "        T.evaluate(0)\n",
    "\n",
    "    @T.prim_func\n",
    "    def log(\n",
    "        rxplaceholder: T.Buffer(T.int64(10), \"float32\"),\n",
    "        compute: T.Buffer(T.int64(10), \"float32\"),\n",
    "    ):\n",
    "        T.evaluate(0)\n",
    "\n",
    "    @T.prim_func\n",
    "    def exp(\n",
    "        rxplaceholder: T.Buffer((T.int64(2), T.int64(4)), \"float32\"),\n",
    "        compute: T.Buffer((T.int64(2), T.int64(4)), \"float32\"),\n",
    "    ):\n",
    "        T.evaluate(0)\n",
    "\n",
    "    @T.prim_func\n",
    "    def pad(\n",
    "        rxplaceholder: T.Buffer(T.int64(8), \"float32\"),\n",
    "        PadInput: T.Buffer(T.int64(10), \"float32\"),\n",
    "    ):\n",
    "        T.evaluate(0)\n",
    "\n",
    "    @R.function\n",
    "    def main(x: R.Tensor((2, 4), dtype=\"float32\")) -> R.Tensor((10,), dtype=\"float32\"):\n",
    "        cls = Module\n",
    "        storage: R.Object = R.memory.alloc_storage(\n",
    "            R.shape([32]), virtual_device_index=0, storage_scope=\"global\", dtype=\"float32\"\n",
    "        )\n",
    "        alloc: R.Tensor((2, 4), dtype=\"float32\") = R.memory.alloc_tensor(\n",
    "            storage, offset=0, shape=R.shape([2, 4]), dtype=\"float32\"\n",
    "        )\n",
    "        _: R.Tuple() = cls.exp(x, alloc)\n",
    "        lv: R.Tensor((2, 4), dtype=\"float32\") = alloc\n",
    "        lv1: R.Tensor((8,), dtype=\"float32\") = R.call_packed(\n",
    "            \"vm.builtin.reshape\", lv, R.shape([8]), sinfo_args=[R.Tensor((8,), dtype=\"float32\")]\n",
    "        )\n",
    "        storage1: R.Object = R.memory.alloc_storage(\n",
    "            R.shape([40]), virtual_device_index=0, storage_scope=\"global\", dtype=\"float32\"\n",
    "        )\n",
    "        alloc1: R.Tensor((8,), dtype=\"float32\") = R.memory.alloc_tensor(\n",
    "            storage1, offset=0, shape=R.shape([8]), dtype=\"float32\"\n",
    "        )\n",
    "        _1: R.Tuple() = cls.relu(lv1, alloc1)\n",
    "        _2: R.Tuple() = R.memory.kill_tensor(alloc)\n",
    "        _3: R.Tuple() = R.memory.kill_tensor(lv1)\n",
    "        lv2: R.Tensor((8,), dtype=\"float32\") = alloc1\n",
    "        alloc2: R.Tensor((8,), dtype=\"float32\") = R.memory.alloc_tensor(\n",
    "            storage, offset=0, shape=R.shape([8]), dtype=\"float32\"\n",
    "        )\n",
    "        _4: R.Tuple() = cls.add(lv2, R.const(1, \"float32\"), alloc2)\n",
    "        _5: R.Tuple() = R.memory.kill_tensor(alloc1)\n",
    "        lv3: R.Tensor((8,), dtype=\"float32\") = alloc2\n",
    "        alloc3: R.Tensor((10,), dtype=\"float32\") = R.memory.alloc_tensor(\n",
    "            storage1, offset=0, shape=R.shape([10]), dtype=\"float32\"\n",
    "        )\n",
    "        _6: R.Tuple() = cls.pad(lv3, alloc3)\n",
    "        _7: R.Tuple() = R.memory.kill_tensor(alloc2)\n",
    "        lv4: R.Tensor((10,), dtype=\"float32\") = alloc3\n",
    "        alloc4: R.Tensor((10,), dtype=\"float32\") = R.builtin.alloc_tensor(\n",
    "            R.shape([10]), dtype=\"float32\", runtime_device_index=0\n",
    "        )\n",
    "        _8: R.Tuple() = cls.log(lv4, alloc4)\n",
    "        _9: R.Tuple() = R.memory.kill_tensor(alloc3)\n",
    "        gv5: R.Tensor((10,), dtype=\"float32\") = alloc4\n",
    "        _11: R.Tuple() = R.memory.kill_storage(storage)\n",
    "        _10: R.Tuple() = R.memory.kill_storage(storage1)\n",
    "        return gv5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "        estimate_memory_usage(Module)\n",
    "        == r\"\"\"Memory usage estimation:\n",
    "- Function main:\n",
    " * Without memory planning, there are 5 constant-size memory allocation(s) with total size 1.639e-07 GB.\n",
    " * With memory planning, there are 2 constant-size memory allocation(s) with total size 6.706e-08 GB.\n",
    " * Memory planning reduces constant memory size to 40.9%.\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
