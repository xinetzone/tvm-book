{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACs 计算\n",
    "\n",
    "粗略地计算模型的 MACs（Multiply-Accumulate operations）的数量。只有 CONV 和 Dense ops 中的 MAC 被计算在内。此 pass（`tvm/src/relay/analysis/mac_count.cc`）在类型推断 pass 被调用后有效，否则计数为 `0`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.relay import analysis, transform\n",
    "\n",
    "\n",
    "def run_opt_pass(expr, opt_pass):\n",
    "    assert isinstance(opt_pass, tvm.transform.Pass)\n",
    "    mod = tvm.IRModule.from_expr(expr)\n",
    "    mod = tvm.relay.transform.InferType()(mod)\n",
    "    mod = opt_pass(mod)\n",
    "    entry = mod[\"main\"]\n",
    "    return entry if isinstance(expr, relay.Function) else entry.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:28] /media/pc/data/lxw/ai/tvm/src/relay/analysis/mac_count.cc:172: This pass only counts MACs in direct conv2d, conv2d_transpose, dense, and batch_matmul ops\n",
      "[14:54:28] /media/pc/data/lxw/ai/tvm/src/relay/analysis/mac_count.cc:172: This pass only counts MACs in direct conv2d, conv2d_transpose, dense, and batch_matmul ops\n",
      "[14:54:28] /media/pc/data/lxw/ai/tvm/src/relay/analysis/mac_count.cc:172: This pass only counts MACs in direct conv2d, conv2d_transpose, dense, and batch_matmul ops\n",
      "[14:54:28] /media/pc/data/lxw/ai/tvm/src/relay/analysis/mac_count.cc:172: This pass only counts MACs in direct conv2d, conv2d_transpose, dense, and batch_matmul ops\n",
      "[14:54:28] /media/pc/data/lxw/ai/tvm/src/relay/analysis/mac_count.cc:172: This pass only counts MACs in direct conv2d, conv2d_transpose, dense, and batch_matmul ops\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_gemm():\n",
    "    n = 512\n",
    "    k = 1024\n",
    "    m = 256\n",
    "    dshape1 = (n, k)\n",
    "    dshape2 = (m, k)\n",
    "    data1 = relay.var(\"data1\", shape=dshape1)\n",
    "    data2 = relay.var(\"data2\", shape=dshape2)\n",
    "    gemm = relay.nn.dense(data1, data2)\n",
    "    func = relay.Function([data1, data2], relay.Tuple(tvm.runtime.convert([gemm])))\n",
    "    func = run_opt_pass(func, transform.InferType())\n",
    "    compute_count = analysis.get_total_mac_number(func)\n",
    "    expect_count = n * m * k\n",
    "    assert compute_count == expect_count\n",
    "\n",
    "\n",
    "def test_conv():\n",
    "    batch_size = 1\n",
    "    input_channel = 3\n",
    "    h = 224\n",
    "    w = 224\n",
    "    output_channel = 64\n",
    "    kh = 7\n",
    "    kw = 7\n",
    "    h_padding = 1\n",
    "    w_padding = 1\n",
    "    oh = h + h_padding * 2 - kh + 1\n",
    "    ow = w + w_padding * 2 - kw + 1\n",
    "    dshape = (batch_size, input_channel, h, w)\n",
    "    weight = relay.var(\"weight\", shape=(output_channel, input_channel, kh, kw))\n",
    "    data = relay.var(\"data\", shape=dshape)\n",
    "    conv2d = relay.nn.conv2d(\n",
    "        data, weight, channels=output_channel, kernel_size=(kh, kw), padding=(h_padding, w_padding)\n",
    "    )\n",
    "    func = relay.Function([data, weight], relay.Tuple(tvm.runtime.convert([conv2d])))\n",
    "    func = run_opt_pass(func, transform.InferType())\n",
    "    compute_count = analysis.get_total_mac_number(func)\n",
    "    expect_count = batch_size * input_channel * oh * ow * output_channel * kh * kw\n",
    "    assert compute_count == expect_count\n",
    "\n",
    "\n",
    "def test_simple_network():\n",
    "    batch_size = 1\n",
    "    dshape = (batch_size, 64, 56, 56)\n",
    "    weight_conv = relay.var(\"weight_conv\", shape=(64, 64, 3, 3))\n",
    "    data1 = relay.var(\"data1\", shape=dshape)\n",
    "    data2 = relay.var(\"data2\", shape=dshape)\n",
    "    weight_dense = relay.var(\"weight_dense\", shape=(1, 56 * 56 * 64))\n",
    "\n",
    "    conv2d_1 = relay.nn.conv2d(data1, weight_conv, channels=64, kernel_size=(3, 3), padding=(1, 1))\n",
    "    conv2d_2 = relay.nn.conv2d(data2, weight_conv, channels=64, kernel_size=(3, 3), padding=(1, 1))\n",
    "    add = relay.add(conv2d_1, conv2d_2)\n",
    "    flattened = relay.nn.batch_flatten(add)\n",
    "    dense_1 = relay.nn.dense(flattened, weight_dense)\n",
    "\n",
    "    func = relay.Function(\n",
    "        [data1, data2, weight_conv, weight_dense],\n",
    "        relay.Tuple(tvm.runtime.convert([conv2d_1, conv2d_2, dense_1, add, flattened])),\n",
    "    )\n",
    "    # alter the CONV 2D data layout to test\n",
    "    func = run_opt_pass(func, transform.AlterOpLayout())\n",
    "    compute_count = analysis.get_total_mac_number(func)\n",
    "    expect_count = 231411712\n",
    "    assert compute_count == expect_count\n",
    "\n",
    "\n",
    "def test_depthwise_conv2d():\n",
    "    batch_size = 1\n",
    "    dshape = (batch_size, 64, 56, 56)\n",
    "    weight_conv = relay.var(\"weight_depthwiseconv\", shape=(64, 1, 3, 3))\n",
    "    data1 = relay.var(\"data1\", shape=dshape)\n",
    "    data2 = relay.var(\"data2\", shape=dshape)\n",
    "    depthwise_conv2d_1 = relay.nn.conv2d(\n",
    "        data1, weight_conv, kernel_size=(3, 3), padding=(1, 1), groups=64\n",
    "    )\n",
    "    depthwise_conv2d_2 = relay.nn.conv2d(\n",
    "        data2, weight_conv, kernel_size=(3, 3), padding=(1, 1), groups=64\n",
    "    )\n",
    "    add = relay.add(depthwise_conv2d_1, depthwise_conv2d_2)\n",
    "    func = relay.Function(\n",
    "        [data1, data2, weight_conv],\n",
    "        relay.Tuple(tvm.runtime.convert([depthwise_conv2d_1, depthwise_conv2d_2, add])),\n",
    "    )\n",
    "    func = run_opt_pass(func, transform.InferType())\n",
    "    compute_count = analysis.get_total_mac_number(func)\n",
    "    assert compute_count == 2 * np.prod(dshape) * 3 * 3\n",
    "\n",
    "\n",
    "def test_conv_2d_transpose():\n",
    "    batch_size = 1\n",
    "    input_channel = 3\n",
    "    h = 224\n",
    "    w = 224\n",
    "    output_channel = 64\n",
    "    kh = 7\n",
    "    kw = 7\n",
    "    h_padding = 1\n",
    "    w_padding = 1\n",
    "    oh = h - h_padding * 2 + kh - 1\n",
    "    ow = w - w_padding * 2 + kw - 1\n",
    "    dshape = (batch_size, input_channel, h, w)\n",
    "    weight = relay.var(\"weight\", shape=(input_channel, output_channel, kh, kw))\n",
    "    data = relay.var(\"data\", shape=dshape)\n",
    "    conv2d_transpose = relay.nn.conv2d_transpose(\n",
    "        data, weight, channels=output_channel, kernel_size=(kh, kw), padding=(h_padding, w_padding)\n",
    "    )\n",
    "    func = relay.Function([data, weight], relay.Tuple(tvm.runtime.convert([conv2d_transpose])))\n",
    "    func = run_opt_pass(func, transform.InferType())\n",
    "    compute_count = analysis.get_total_mac_number(func)\n",
    "    expect_count = batch_size * input_channel * oh * ow * output_channel * kh * kw\n",
    "    assert compute_count == expect_count\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_conv()\n",
    "    test_gemm()\n",
    "    test_simple_network()\n",
    "    test_depthwise_conv2d()\n",
    "    test_conv_2d_transpose()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConvMacCount:\n",
    "    call_node: relay.expr.Call\n",
    "\n",
    "    @property\n",
    "    def count(self):\n",
    "        args = self.call_node.args\n",
    "        assert len(args) == 2, \"CONV 2D 节点的输入参数个数应为 2。\"\n",
    "        conv_2d_attr = self.call_node.attrs\n",
    "        data_type = args[0].checked_type\n",
    "        data_shape = data_type.shape\n",
    "        # data_layout = conv_2d_attr.data_layout\n",
    "        input_channel = data_shape[0]\n",
    "        kernel_size = conv_2d_attr.kernel_size\n",
    "        assert len(kernel_size)==2, \"Conv 2D 中的 kernel 的维数应该是 2。\"\n",
    "        expr = self.call_node.checked_type\n",
    "        output_tensor = expr.shape\n",
    "        if len(output_tensor) in [4, 5]:\n",
    "            _count = np.prod(list(output_tensor)) * np.prod(list(kernel_size))\n",
    "        _count *= int(int(input_channel) / int(conv_2d_attr.groups))\n",
    "        return _count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34e95b0948f576614c7863cc780d83f61f9551597d4ec05ab5fbb4cfe73deb20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
