{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target-codegen-vm-basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import te\n",
    "from tvm.script import tir as T, ir as I\n",
    "import numpy as np\n",
    "import tvm.testing\n",
    "\n",
    "def run_jit(fapi, check):\n",
    "    for target in [\"llvm\", \"stackvm\"]:\n",
    "        if not tvm.testing.device_enabled(target):\n",
    "            continue\n",
    "        f = tvm.driver.build(fapi, target=target)\n",
    "        s = f.get_source()\n",
    "        check(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stack_vm_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:52:58] /media/pc/data/lxw/ai/tvm/src/target/target_kind.cc:181: Warning: Unable to detect CUDA version, default to \"-arch=sm_50\" instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:52:59] /media/pc/data/lxw/ai/tvm/src/target/target_kind.cc:181: Warning: Unable to detect CUDA version, default to \"-arch=sm_50\" instead\n"
     ]
    }
   ],
   "source": [
    "a = tvm.nd.array(np.zeros(10, dtype=\"float32\"))\n",
    "\n",
    "@tvm.register_func\n",
    "def tvm_call_back_get_shape(shape0):\n",
    "    print(shape0)\n",
    "    assert shape0 == a.shape[0]\n",
    "\n",
    "n = te.size_var(\"n\")\n",
    "Ab = tvm.tir.decl_buffer((n,), \"float32\")\n",
    "stmt = tvm.tir.Evaluate(tvm.tir.call_packed(\"tvm_call_back_get_shape\", Ab.shape[0]))\n",
    "\n",
    "mod = tvm.IRModule.from_expr(\n",
    "    tvm.tir.PrimFunc([Ab], stmt).with_attr(\"global_symbol\", \"print_shape\")\n",
    ")\n",
    "\n",
    "run_jit(mod, lambda f: f(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stack_vm_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n",
      "(1,)\n",
      "(2,)\n",
      "(3,)\n",
      "(4,)\n",
      "(5,)\n",
      "(6,)\n",
      "(7,)\n",
      "(8,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:53:03] /media/pc/data/lxw/ai/tvm/src/target/target_kind.cc:181: Warning: Unable to detect CUDA version, default to \"-arch=sm_50\" instead\n",
      "[23:53:04] /media/pc/data/lxw/ai/tvm/src/target/target_kind.cc:181: Warning: Unable to detect CUDA version, default to \"-arch=sm_50\" instead\n"
     ]
    }
   ],
   "source": [
    "@tvm.register_func\n",
    "def tvm_stack_vm_print(*x):\n",
    "    print(x)\n",
    "\n",
    "dtype = \"int64\"\n",
    "n = te.size_var(\"n\")\n",
    "Ab = tvm.tir.decl_buffer((n,), dtype)\n",
    "i = te.size_var(\"i\")\n",
    "\n",
    "ib = tvm.tir.ir_builder.create()\n",
    "A = ib.buffer_ptr(Ab)\n",
    "with ib.for_range(0, n - 1, \"i\") as i:\n",
    "    A[i + 1] = A[i] + 1\n",
    "    ib.emit(tvm.tir.call_packed(\"tvm_stack_vm_print\", i))\n",
    "\n",
    "stmt = ib.get()\n",
    "mod = tvm.IRModule.from_expr(tvm.tir.PrimFunc([Ab], stmt).with_attr(\"global_symbol\", \"ramp\"))\n",
    "a = tvm.nd.array(np.zeros(10, dtype=dtype))\n",
    "\n",
    "def check(f):\n",
    "    f(a)\n",
    "    np.testing.assert_equal(a.numpy(), np.arange(a.shape[0]))\n",
    "\n",
    "run_jit(mod, check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stack_vm_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:53:47] /media/pc/data/lxw/ai/tvm/src/target/target_kind.cc:181: Warning: Unable to detect CUDA version, default to \"-arch=sm_50\" instead\n",
      "[23:53:47] /media/pc/data/lxw/ai/tvm/src/target/target_kind.cc:181: Warning: Unable to detect CUDA version, default to \"-arch=sm_50\" instead\n"
     ]
    }
   ],
   "source": [
    "dtype = \"int64\"\n",
    "n = te.size_var(\"n\")\n",
    "Ab = tvm.tir.decl_buffer((n,), dtype)\n",
    "\n",
    "ib = tvm.tir.ir_builder.create()\n",
    "A = ib.buffer_ptr(Ab)\n",
    "with ib.for_range(0, n - 1, \"i\") as i:\n",
    "    with ib.if_scope(tvm.tir.EQ(i, 4)):\n",
    "        A[i + 1] = A[i] + 1\n",
    "    with ib.else_scope():\n",
    "        A[i + 1] = A[i] + 2\n",
    "\n",
    "stmt = ib.get()\n",
    "mod = tvm.IRModule.from_expr(tvm.tir.PrimFunc([Ab], stmt).with_attr(\"global_symbol\", \"test\"))\n",
    "\n",
    "def check(f):\n",
    "    a = tvm.nd.array(np.zeros(10, dtype=dtype))\n",
    "    f(a)\n",
    "    y = np.arange(a.shape[0]) * 2\n",
    "    y[5:] -= 1\n",
    "    np.testing.assert_equal(a.numpy(), y)\n",
    "\n",
    "run_jit(mod, check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vm_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:54:37] /media/pc/data/lxw/ai/tvm/src/target/target_kind.cc:181: Warning: Unable to detect CUDA version, default to \"-arch=sm_50\" instead\n",
      "[23:54:37] /media/pc/data/lxw/ai/tvm/src/target/target_kind.cc:181: Warning: Unable to detect CUDA version, default to \"-arch=sm_50\" instead\n"
     ]
    }
   ],
   "source": [
    "dtype = \"int64\"\n",
    "n = te.size_var(\"n\")\n",
    "Ab = tvm.tir.decl_buffer((n,), dtype)\n",
    "i = te.size_var(\"i\")\n",
    "ib = tvm.tir.ir_builder.create()\n",
    "A = ib.buffer_ptr(Ab)\n",
    "with ib.for_range(0, n, \"i\", kind=\"parallel\") as i:\n",
    "    A[i] = A[i] + 1\n",
    "stmt = ib.get()\n",
    "mod = tvm.IRModule.from_expr(tvm.tir.PrimFunc([Ab], stmt).with_attr(\"global_symbol\", \"test\"))\n",
    "\n",
    "def check(f):\n",
    "    a = tvm.nd.array(np.zeros(10, dtype=dtype))\n",
    "    f(a)\n",
    "    np.testing.assert_equal(a.numpy(), np.ones(a.shape[0]))\n",
    "\n",
    "run_jit(mod, check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## codegen_decl_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Module(stackvm, 45f10a8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The codegen should accept DeclBuffer nodes in its input\n",
    "@I.ir_module\n",
    "class mod:\n",
    "    @T.prim_func\n",
    "    def kernel(A_data: T.handle(\"float32\")):\n",
    "        T.func_attr({\"global_symbol\": \"kernel\"})\n",
    "        A_buf = T.decl_buffer([256], dtype=\"float32\", scope=\"global\", data=A_data)\n",
    "\n",
    "target = tvm.target.Target(\"stackvm\")\n",
    "stackvm_codegen = tvm.get_global_func(\"target.build.stackvm\")\n",
    "stackvm_codegen(mod, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
