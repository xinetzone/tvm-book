{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算图分割(old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm.relay.backend import te_compiler\n",
    "from tvm.relay.backend.runtime import Runtime\n",
    "import tvm.relay.testing\n",
    "import tvm.relay.op as reg\n",
    "from tvm import relay\n",
    "from tvm.relay import transform\n",
    "from tvm.relay.testing import byoc\n",
    "from tvm.contrib import utils\n",
    "from tvm.relay.expr_functor import ExprMutator\n",
    "from tvm.relay.op.annotation import compiler_begin, compiler_end\n",
    "from tvm.relay.op.contrib.register import get_pattern_table\n",
    "from tvm.relay.build_module import bind_params_by_name\n",
    "\n",
    "def set_func_attr(func, compile_name, symbol_name):\n",
    "    func = func.with_attr(\"Primitive\", tvm.tir.IntImm(\"int32\", 1))\n",
    "    func = func.with_attr(\"Inline\", tvm.tir.IntImm(\"int32\", 1))\n",
    "    func = func.with_attr(\"Compiler\", compile_name)\n",
    "    func = func.with_attr(\"global_symbol\", symbol_name)\n",
    "    return func\n",
    "\n",
    "def update_lib(lib, source_dir=\"/media/pc/data/lxw/ai/tvm\"):\n",
    "    kwargs = {\n",
    "        \"options\" : [\n",
    "            \"-O2\", \"-std=c++17\", \n",
    "            f\"-I{source_dir}/src/runtime/contrib\", \n",
    "            f\"-I{source_dir}/include\",\n",
    "            f\"-I{source_dir}/3rdparty/dlpack/include\",\n",
    "            f\"-I{source_dir}/3rdparty/dmlc-core/include\",\n",
    "        ]\n",
    "    }\n",
    "    tmp_path = utils.tempdir()\n",
    "    lib_name = \"lib.so\"\n",
    "    lib_path = tmp_path.relpath(lib_name)\n",
    "    lib.export_library(lib_path, fcompile=False, **kwargs)\n",
    "    lib = tvm.runtime.load_module(lib_path)\n",
    "    return lib\n",
    "\n",
    "\n",
    "class MobileNetAnnotator(ExprMutator):\n",
    "    \"\"\"\n",
    "    Annotate mobilenet until global_avg_pool.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, compiler):\n",
    "        super(MobileNetAnnotator, self).__init__()\n",
    "        self.compiler = compiler\n",
    "        self.compiler_open = False\n",
    "\n",
    "    def visit_call(self, call):\n",
    "\n",
    "        if call.op.name == \"nn.global_avg_pool2d\":\n",
    "            self.compiler_open = True\n",
    "        compiler_open = self.compiler_open\n",
    "\n",
    "        params = []\n",
    "        for arg in call.args:\n",
    "            param = super().visit(arg)\n",
    "            if call.op.name == \"nn.global_avg_pool2d\":\n",
    "                param = compiler_end(param, self.compiler)\n",
    "            if compiler_open and isinstance(param, relay.expr.Var):\n",
    "                param = compiler_begin(param, self.compiler)\n",
    "            params.append(param)\n",
    "\n",
    "        new_call = relay.Call(call.op, params, call.attrs)\n",
    "        return new_call\n",
    "\n",
    "def check_result(\n",
    "    mod,\n",
    "    map_inputs,\n",
    "    out_shape,\n",
    "    result,\n",
    "    tol=1e-5,\n",
    "    target=\"llvm\",\n",
    "    device=tvm.cpu(),\n",
    "    params=None,\n",
    "    runtime=Runtime(\"cpp\"),\n",
    "):\n",
    "    def check_vm_result():\n",
    "        te_compiler.get().clear()\n",
    "        with tvm.transform.PassContext(opt_level=3):\n",
    "            exe = relay.vm.compile(mod, target=target, params=params)\n",
    "        code, lib = exe.save()\n",
    "        lib = update_lib(lib)\n",
    "        exe = tvm.runtime.vm.Executable.load_exec(code, lib)\n",
    "        vm = tvm.runtime.vm.VirtualMachine(exe, device)\n",
    "        outs = vm.run(**map_inputs)\n",
    "        outs = outs if isinstance(outs, tvm.runtime.container.ADT) else [outs]\n",
    "        results = result if isinstance(result, list) else [result]\n",
    "        for out, ref in zip(outs, results):\n",
    "            tvm.testing.assert_allclose(out.numpy(), ref, rtol=tol, atol=tol)\n",
    "    check_vm_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mixed_single_multiple_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph():\n",
    "    data = relay.var(\"data\", shape=(10, 10))\n",
    "\n",
    "    cb_1 = compiler_begin(data, \"test_target\")\n",
    "    O_1 = relay.abs(cb_1)\n",
    "    ce_2 = compiler_end(O_1, \"test_target\")\n",
    "    O_2 = relay.nn.relu(O_1)\n",
    "    ce_3 = compiler_end(O_2, \"test_target\")\n",
    "\n",
    "    X = relay.tanh(ce_2)\n",
    "\n",
    "    cb_3 = compiler_begin(ce_3, \"test_target\")\n",
    "    cb_4 = compiler_begin(X, \"test_target\")\n",
    "    O_3 = relay.add(cb_3, cb_4)\n",
    "    ce_4 = compiler_end(O_3, \"test_target\")\n",
    "\n",
    "    func = relay.Function([data], ce_4)\n",
    "    return func\n",
    "\n",
    "mod = tvm.IRModule()\n",
    "mod[\"main\"] = create_graph()\n",
    "mod = transform.InferType()(mod)\n",
    "\n",
    "partitioned = transform.PartitionGraph()(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple_use_of_an_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_multiple_use_of_an_output():\n",
    "    def expected_same_output_region():\n",
    "        mod = tvm.IRModule()\n",
    "        x = relay.var(\"x\", shape=(8, 8))\n",
    "        y = relay.var(\"y\", shape=(8, 8))\n",
    "        z = relay.var(\"z\", shape=(8, 8))\n",
    "        x0 = relay.var(\"x0\", shape=(8, 8))\n",
    "        y0 = relay.var(\"y0\", shape=(8, 8))\n",
    "        log = relay.log(x0)\n",
    "        sub = x0 - y0\n",
    "        mul = log * sub\n",
    "        # The partitioned graph contains log, subtract, and multiply\n",
    "        func = relay.Function([x0, y0], mul)\n",
    "        func = set_func_attr(func, \"ccompiler\", \"tvmgen_default_ccompiler_main_0\")\n",
    "        glb_0 = relay.GlobalVar(\"tvmgen_default_ccompiler_main_0\")\n",
    "        mod[glb_0] = func\n",
    "        mod = transform.InferType()(mod)\n",
    "\n",
    "        add = x + y\n",
    "        call = relay.Call(glb_0, [add, z])\n",
    "        main = relay.Function([x, y, z], call)\n",
    "        mod[\"main\"] = main\n",
    "        mod = transform.InferType()(mod)\n",
    "        return mod\n",
    "\n",
    "    def expected_different_output_region():\n",
    "        mod = tvm.IRModule()\n",
    "        x = relay.var(\"x\", shape=(8, 8))\n",
    "        y = relay.var(\"y\", shape=(8, 8))\n",
    "        z = relay.var(\"z\", shape=(8, 8))\n",
    "\n",
    "        # The partitioned graph contains log\n",
    "        i0 = relay.var(\"i0\", shape=(8, 8))\n",
    "        log = relay.log(i0)\n",
    "        func = relay.Function([i0], log)\n",
    "        func = set_func_attr(func, \"ccompiler\", \"tvmgen_default_ccompiler_main_0\")\n",
    "        glb_0 = relay.GlobalVar(\"tvmgen_default_ccompiler_main_0\")\n",
    "        mod[glb_0] = func\n",
    "        mod = transform.InferType()(mod)\n",
    "\n",
    "        # The partitioned graph contains subtract\n",
    "        x0 = relay.var(\"x0\", shape=(8, 8))\n",
    "        y0 = relay.var(\"y0\", shape=(8, 8))\n",
    "        sub = x0 - y0\n",
    "        func = relay.Function([x0, y0], sub)\n",
    "        func = set_func_attr(func, \"ccompiler\", \"tvmgen_default_ccompiler_main_1\")\n",
    "        glb_1 = relay.GlobalVar(\"tvmgen_default_ccompiler_main_1\")\n",
    "        mod[glb_1] = func\n",
    "        mod = transform.InferType()(mod)\n",
    "\n",
    "        add = x + y\n",
    "        call_log = relay.Call(glb_0, [add])\n",
    "        call_sub = relay.Call(glb_1, [add, z])\n",
    "        main = relay.Function([x, y, z], call_log * call_sub)\n",
    "        mod[\"main\"] = main\n",
    "        mod = transform.InferType()(mod)\n",
    "        return mod\n",
    "\n",
    "    def get_mod():\n",
    "        x = relay.var(\"x\", shape=(8, 8))\n",
    "        y = relay.var(\"y\", shape=(8, 8))\n",
    "        z = relay.var(\"z\", shape=(8, 8))\n",
    "        add = x + y\n",
    "        sub = add - z\n",
    "        log = relay.log(add)\n",
    "        sub1 = log * sub\n",
    "        f = relay.Function([x, y, z], sub1)\n",
    "        mod = tvm.IRModule()\n",
    "        mod[\"main\"] = f\n",
    "        return mod\n",
    "\n",
    "    def test_same_output_region():\n",
    "        mod = get_mod()\n",
    "        mod = AllowedListAnnotator([\"subtract\", \"log\", \"multiply\"], \"ccompiler\")(mod)\n",
    "        mod = transform.MergeCompilerRegions()(mod)\n",
    "        mod = transform.PartitionGraph()(mod)\n",
    "\n",
    "        expected_mod = expected_same_output_region()\n",
    "        assert tvm.ir.structural_equal(mod, expected_mod, map_free_vars=True)\n",
    "\n",
    "    def test_different_output_region():\n",
    "        mod = get_mod()\n",
    "        mod = AllowedListAnnotator([\"subtract\", \"log\"], \"ccompiler\")(mod)\n",
    "        mod = transform.MergeCompilerRegions()(mod)\n",
    "        mod = transform.PartitionGraph()(mod)\n",
    "\n",
    "        expected_mod = expected_different_output_region()\n",
    "        assert tvm.ir.structural_equal(mod, expected_mod, map_free_vars=True)\n",
    "\n",
    "    test_same_output_region()\n",
    "    test_different_output_region()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multiple_use_of_an_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_duplicate_outputs():\n",
    "    target = \"test_duplicate_outputs\"\n",
    "\n",
    "    @tvm.ir.register_op_attr(\"abs\", \"target.\" + target)\n",
    "    def abs(expr):  # pylint: disable=unused-variable\n",
    "        return True\n",
    "\n",
    "    def create_graph():\n",
    "        data = relay.var(\"data\", shape=(10, 10))\n",
    "        x = relay.abs(data)\n",
    "        out_1 = relay.nn.relu(x)\n",
    "        out_2 = relay.tanh(x)\n",
    "        out_3 = relay.log(x)\n",
    "        out = relay.Tuple([out_1, out_2, out_3])\n",
    "        func = relay.Function([data], out)\n",
    "        return func\n",
    "\n",
    "    def expected():\n",
    "        mod = tvm.IRModule()\n",
    "\n",
    "        # function 0\n",
    "        f0_i0 = relay.var(target + \"_0_i0\", shape=(10, 10))\n",
    "        f0_o0 = relay.abs(f0_i0)\n",
    "        func0 = relay.Function([f0_i0], f0_o0)\n",
    "\n",
    "        func0 = func0.with_attr(\"Primitive\", tvm.tir.IntImm(\"int32\", 1))\n",
    "        func0 = func0.with_attr(\"Inline\", tvm.tir.IntImm(\"int32\", 1))\n",
    "        func0 = func0.with_attr(\"Compiler\", target)\n",
    "        func0 = func0.with_attr(\"global_symbol\", \"tvmgen_default_\" + target + \"_main_0\")\n",
    "        gv0 = relay.GlobalVar(\"tvmgen_default_\" + target + \"_main_0\")\n",
    "        mod[gv0] = func0\n",
    "        mod = transform.InferType()(mod)\n",
    "\n",
    "        # body\n",
    "        data = relay.var(\"data\", shape=(10, 10))\n",
    "        function_out = gv0(data)\n",
    "        out_1 = relay.nn.relu(function_out)\n",
    "        out_2 = relay.tanh(function_out)\n",
    "        out_3 = relay.log(function_out)\n",
    "        out = relay.Tuple([out_1, out_2, out_3])\n",
    "        func = relay.Function([data], out)\n",
    "        mod[\"main\"] = func\n",
    "        mod = transform.InferType()(mod)\n",
    "        return mod\n",
    "\n",
    "    mod = tvm.IRModule()\n",
    "    mod[\"main\"] = create_graph()\n",
    "\n",
    "    seq = tvm.transform.Sequential(\n",
    "        [\n",
    "            transform.AnnotateTarget(target),\n",
    "            transform.MergeCompilerRegions(),\n",
    "            transform.PartitionGraph(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ref_mod = expected()\n",
    "    partitioned = seq(mod)\n",
    "    assert tvm.ir.structural_equal(partitioned, ref_mod, map_free_vars=True)\n",
    "\n",
    "test_duplicate_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_constant_tuples():\n",
    "    @tvm.ir.register_op_attr(\"qnn.concatenate\", \"target.const_tuples\")\n",
    "    def add(expr):  # pylint: disable=unused-variable\n",
    "        return True\n",
    "\n",
    "    def create_graph():\n",
    "        a = relay.var(\"a\", shape=(10, 10), dtype=\"uint8\")\n",
    "        b = relay.var(\"b\", shape=(10, 10), dtype=\"uint8\")\n",
    "        a1 = relay.abs(a)\n",
    "\n",
    "        zeroi = relay.const(1, \"int32\")\n",
    "        zerof = relay.const(0, \"float32\")\n",
    "        con = relay.qnn.op.concatenate(\n",
    "            (a1, b),\n",
    "            input_scales=(zerof, zerof),\n",
    "            input_zero_points=(zeroi, zeroi),\n",
    "            output_scale=zerof,\n",
    "            output_zero_point=zeroi,\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        f = relay.Function([a, b], con)\n",
    "        mod = tvm.IRModule.from_expr(f)\n",
    "        mod = transform.InferType()(mod)\n",
    "        return mod\n",
    "\n",
    "    seq = tvm.transform.Sequential(\n",
    "        [\n",
    "            transform.AnnotateTarget(\"const_tuples\"),\n",
    "            transform.InferType(),\n",
    "            transform.MergeCompilerRegions(),\n",
    "            transform.PartitionGraph(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    partitioned = seq(create_graph())\n",
    "\n",
    "    concat = partitioned[\"tvmgen_default_const_tuples_main_0\"].body\n",
    "    assert type(concat.args[1]) == relay.Tuple\n",
    "    assert type(concat.args[2]) == relay.Tuple\n",
    "    assert type(concat.args[3]) == relay.Constant\n",
    "    assert type(concat.args[4]) == relay.Constant\n",
    "\n",
    "\n",
    "def test_flatten_tuple_output():\n",
    "    target = \"test_flatten_tuple_output\"\n",
    "\n",
    "    @tvm.ir.register_op_attr(\"split\", \"target.\" + target)\n",
    "    def split(expr):  # pylint: disable=unused-variable\n",
    "        return True\n",
    "\n",
    "    @tvm.ir.register_op_attr(\"abs\", \"target.\" + target)\n",
    "    def abs(expr):  # pylint: disable=unused-variable\n",
    "        return True\n",
    "\n",
    "    def create_graph():\n",
    "        a = relay.var(\"a\", shape=(10, 10), dtype=\"uint8\")\n",
    "\n",
    "        a_split = relay.split(a, 2)\n",
    "        a_split_0 = relay.TupleGetItem(a_split.astuple(), 0)\n",
    "        a_split_0_abs = relay.abs(a_split_0)\n",
    "\n",
    "        a_con = relay.concatenate(a_split, 0)\n",
    "        a_split_0_relu = relay.nn.relu(a_split_0_abs)\n",
    "\n",
    "        out = relay.Tuple((a_con, a_split_0_relu))\n",
    "        f = relay.Function([a], out)\n",
    "        mod = tvm.IRModule.from_expr(f)\n",
    "        mod = transform.InferType()(mod)\n",
    "        return mod\n",
    "\n",
    "    def expected():\n",
    "        mod = tvm.IRModule()\n",
    "\n",
    "        # function 0\n",
    "        f0_i0 = relay.var(target + \"_0_i0\", shape=(10, 10), dtype=\"uint8\")\n",
    "        a_split = relay.split(f0_i0, 2)\n",
    "        a_split_0 = relay.TupleGetItem(a_split.astuple(), 0)\n",
    "        a_split_1 = relay.TupleGetItem(a_split.astuple(), 1)\n",
    "        a_split_abs_in = relay.TupleGetItem(a_split.astuple(), 0)\n",
    "        abs = relay.abs(a_split_abs_in)\n",
    "        tuple_out = relay.Tuple((a_split_0, a_split_1, abs))\n",
    "        func0 = relay.Function([f0_i0], tuple_out)\n",
    "\n",
    "        func0 = func0.with_attr(\"Primitive\", tvm.tir.IntImm(\"int32\", 1))\n",
    "        func0 = func0.with_attr(\"Inline\", tvm.tir.IntImm(\"int32\", 1))\n",
    "        func0 = func0.with_attr(\"Compiler\", target)\n",
    "        func0 = func0.with_attr(\"global_symbol\", \"tvmgen_default_\" + target + \"_main_0\")\n",
    "        gv0 = relay.GlobalVar(\"tvmgen_default_\" + target + \"_main_0\")\n",
    "        mod[gv0] = func0\n",
    "        mod = transform.InferType()(mod)\n",
    "\n",
    "        # body\n",
    "        data = relay.var(\"a\", shape=(10, 10), dtype=\"uint8\")\n",
    "        f_out = gv0(data)\n",
    "        f_out_0 = relay.TupleGetItem(f_out, 0)\n",
    "        f_out_1 = relay.TupleGetItem(f_out, 1)\n",
    "        tuple = relay.Tuple((f_out_0, f_out_1))\n",
    "        concat = relay.concatenate(tuple, 0)\n",
    "        f_out_2 = relay.TupleGetItem(f_out, 2)\n",
    "        relu = relay.nn.relu(f_out_2)\n",
    "        ret_tuple = relay.Tuple((concat, relu))\n",
    "        mod[\"main\"] = relay.Function([data], ret_tuple)\n",
    "        mod = transform.InferType()(mod)\n",
    "        return mod\n",
    "\n",
    "    seq = tvm.transform.Sequential(\n",
    "        [\n",
    "            transform.AnnotateTarget(target),\n",
    "            transform.MergeCompilerRegions(),\n",
    "            transform.PartitionGraph(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    partitioned = seq(create_graph())\n",
    "    partitioned = transform.InferType()(partitioned)\n",
    "    expected_mod = transform.InferType()(expected())\n",
    "    assert tvm.ir.structural_equal(partitioned, expected_mod, map_free_vars=True)\n",
    "\n",
    "\n",
    "def test_tuple_output_exec():\n",
    "    \"\"\"Test C codegen and runtime for a subgraph with a tuple output\"\"\"\n",
    "    a = relay.var(\"a\", shape=(10, 10), dtype=\"float32\")\n",
    "    b = relay.var(\"b\", shape=(10, 10), dtype=\"float32\")\n",
    "    ba = relay.annotation.compiler_begin(a, \"ccompiler\")\n",
    "    bb = relay.annotation.compiler_begin(b, \"ccompiler\")\n",
    "    add = relay.add(ba, bb)\n",
    "    sub = relay.subtract(ba, bb)\n",
    "    out = relay.Tuple((add, sub))\n",
    "    eout = relay.annotation.compiler_end(out, \"ccompiler\")\n",
    "    func = relay.Function([a, b], eout)\n",
    "\n",
    "    mod = tvm.IRModule()\n",
    "    mod[\"main\"] = func\n",
    "    mod = transform.InferType()(mod)\n",
    "    mod = transform.PartitionGraph()(mod)\n",
    "\n",
    "    a_data = np.random.rand(10, 10).astype(\"float32\")\n",
    "    b_data = np.random.rand(10, 10).astype(\"float32\")\n",
    "\n",
    "    check_result(\n",
    "        mod,\n",
    "        {\"a\": a_data, \"b\": b_data},\n",
    "        [(10, 10), (10, 10)],\n",
    "        [(a_data + b_data), (a_data - b_data)],\n",
    "    )\n",
    "\n",
    "\n",
    "def test_extern_opt():\n",
    "    def Optimize(mod):\n",
    "        return relay.transform.FoldConstant()(mod)\n",
    "\n",
    "    tvm.register_func(\"relay.ext.test_target.optimize\", Optimize)\n",
    "\n",
    "    x = relay.var(\"x\", shape=(2, 2))\n",
    "    y0 = relay.var(\"y0\", shape=(2, 2))\n",
    "    y1 = relay.var(\"y1\", shape=(2, 2))\n",
    "    yy0 = relay.annotation.compiler_begin(y0, \"test_target\")\n",
    "    yy1 = relay.annotation.compiler_begin(y1, \"test_target\")\n",
    "    z = yy0 + yy1\n",
    "    end = relay.annotation.compiler_end(z, \"test_target\")\n",
    "    f = relay.Function([x, y0, y1], end * x)\n",
    "    c = np.ones(shape=(2, 2), dtype=\"float32\")\n",
    "    f = bind_params_by_name(f, {\"y0\": tvm.nd.array(c), \"y1\": tvm.nd.array(c)})\n",
    "    mod = tvm.IRModule()\n",
    "    mod[\"main\"] = f\n",
    "    mod = transform.InferType()(mod)\n",
    "    mod = transform.PartitionGraph()(mod)\n",
    "\n",
    "    try:\n",
    "        t0 = mod[\"tvmgen_default_test_target_main_0\"]\n",
    "    except:\n",
    "        raise KeyError(\"test_target_main_0 not found\")\n",
    "\n",
    "    assert isinstance(t0.body, relay.Constant)\n",
    "    expected = np.empty([2, 2])\n",
    "    expected.fill(2)\n",
    "    tvm.testing.assert_allclose(t0.body.data.numpy(), expected, rtol=1e-5, atol=1e-5)\n",
    "\n",
    "\n",
    "def test_preserve_type_import():\n",
    "    \"\"\"Test to make sure type definition and imports are preserved during the BYOC pipeline.\"\"\"\n",
    "    from tvm.relay.prelude import Prelude, StaticTensorArrayOps\n",
    "\n",
    "    def run(dtype, shape):\n",
    "        mod = tvm.IRModule()\n",
    "        p = Prelude(mod)\n",
    "        static_tensor_array_ops = StaticTensorArrayOps(p, dtype, shape)\n",
    "        static_tensor_array_ops.register()\n",
    "\n",
    "        tensor_array = p.get_global_var_static(\"tensor_array\", dtype, shape)\n",
    "        tensor = p.get_tensor_ctor_static(\"tensor_constructor\", dtype, shape)\n",
    "        write = p.get_global_var_static(\"tensor_array_write\", dtype, shape)\n",
    "        gather = p.get_global_var_static(\"tensor_array_gather\", dtype, shape)\n",
    "        v = relay.var(\"v\")\n",
    "        indice = relay.var(\"indice\")\n",
    "        init_tensor_array = tensor_array(relay.const(3))\n",
    "        tensor_array1 = write(init_tensor_array, relay.const(0), tensor(v))\n",
    "        tensor_array2 = write(tensor_array1, relay.const(1), tensor(v))\n",
    "        tensor_array3 = write(tensor_array2, relay.const(2), tensor(v))\n",
    "        out = gather(tensor_array3, indice)\n",
    "        mod[\"main\"] = relay.Function([v, indice], out)\n",
    "        mod = transform.RemoveUnusedFunctions()(mod)\n",
    "        mod = transform.PartitionGraph()(mod)\n",
    "\n",
    "    run(\"float32\", [2, 3])\n",
    "\n",
    "\n",
    "def test_not_bind_constant():\n",
    "    def get_net(prefix, data, out_channel):\n",
    "        weight = relay.var(prefix + \"weight\")\n",
    "        bn_gamma = relay.var(prefix + \"bn_gamma\")\n",
    "        bn_beta = relay.var(prefix + \"bn_beta\")\n",
    "        bn_mmean = relay.var(prefix + \"bn_mean\")\n",
    "        bn_mvar = relay.var(prefix + \"bn_var\")\n",
    "\n",
    "        layer = relay.nn.conv2d(\n",
    "            data=data, weight=weight, kernel_size=(3, 3), channels=out_channel, padding=(1, 1)\n",
    "        )\n",
    "        bn_output = relay.nn.batch_norm(layer, bn_gamma, bn_beta, bn_mmean, bn_mvar)\n",
    "        out = relay.nn.relu(bn_output[0])\n",
    "        return relay.Function(relay.analysis.free_vars(out), out)\n",
    "\n",
    "    def get_partitoned_mod(mod, params, pattern_table, bind_constants):\n",
    "        mod[\"main\"] = bind_params_by_name(mod[\"main\"], params)\n",
    "        remove_bn_pass = tvm.transform.Sequential(\n",
    "            [\n",
    "                transform.InferType(),\n",
    "                transform.SimplifyInference(),\n",
    "                transform.FoldConstant(),\n",
    "                transform.FoldScaleAxis(),\n",
    "            ]\n",
    "        )\n",
    "        composite_partition = tvm.transform.Sequential(\n",
    "            [\n",
    "                remove_bn_pass,\n",
    "                transform.MergeComposite(pattern_table),\n",
    "                transform.AnnotateTarget(\"dnnl\"),\n",
    "                transform.PartitionGraph(bind_constants=bind_constants),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        with tvm.transform.PassContext(opt_level=3, disabled_pass=[\"AlterOpLayout\"]):\n",
    "            return composite_partition(mod)\n",
    "\n",
    "    data = relay.var(\"data\", relay.TensorType((1, 3, 224, 224), \"float32\"))\n",
    "    net = get_net(\"block_\", data, 8)\n",
    "    mod, params = tvm.relay.testing.create_workload(net)\n",
    "\n",
    "    mod = get_partitoned_mod(mod, params, get_pattern_table(\"dnnl\"), bind_constants=True)\n",
    "    len(mod[\"main\"].body.args) == 1\n",
    "\n",
    "    mod = get_partitoned_mod(mod, params, get_pattern_table(\"dnnl\"), bind_constants=False)\n",
    "    len(mod[\"main\"].body.args) == 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
