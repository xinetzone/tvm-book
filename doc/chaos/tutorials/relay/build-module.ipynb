{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVM Relay 构建模块\n",
    "\n",
    "参考源代码：\n",
    "\n",
    "- `tvm/python/tvm/relay/build_module.py`\n",
    "- `tvm/src/relay/backend/build_module.cc`\n",
    "\n",
    "了解一个关键函数：\n",
    "\n",
    "```c++\n",
    "TVM_REGISTER_GLOBAL(\"relay.build_module.BindParamsByName\")\n",
    "    .set_body([](TVMArgs args, TVMRetValue* rv) {\n",
    "      Map<String, Constant> params = args[1];\n",
    "      std::unordered_map<std::string, runtime::NDArray> params_;\n",
    "      for (const auto& kv : params) {\n",
    "        params_[kv.first] = kv.second->data;\n",
    "      }\n",
    "      *rv = relay::backend::BindParamsByName(args[0], params_);\n",
    "    });\n",
    "```\n",
    "\n",
    "这段代码是 TVM 中的注册函数，用于将参数定到 Relay 表达式中。\n",
    "\n",
    "函数名为 `relay.build_module.BindParamsByName`，是全局 (global) 函数。TVM 通过使用 `TVM_REGISTER_GLOBAL` 宏来注册该函数。其中，函数的实现是 Lambda 表达式。这个 Lambda 表达式的作用是将函数参数中传入的参数（`args[1]`）所对应的值（`Constant` 类型的 map）中的 NDArray 数据提取出来，并通过 `relay::backend::BindParamsByName` API 将这些数据绑定到传入的 Relay 表达式（`args[0]`）中。提取并绑定数据的过程通过名为 `params_` 的 `unordered_map` 变量实现。\n",
    "\n",
    "TVM 中的 Relay 表达式通常用于描述深度学习模型。这些表达式可以在后端编译和优化之后，生成机器代码，实现对模型的快速预测和推理。在这个过程中，定义模型、定义参数、编译和优化处理这些计算过程中的数据，都需要被有效且高效地绑定到一起。而在这项任务中，`BindParamsByName` API 担当一项重要的作用。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tvm.relay.create_executor`\n",
    "\n",
    "`tvm.relay.create_executor(kind=\"debug\", mod=None, device=None, target=\"llvm\", params=None)`:\n",
    "\n",
    "- `kind: str`：执行器（executor）的类型。`debug` 用于解释器（`interpreter`），`graph` 用于 graph executor，`aot` 用于 aot executor，`vm` 用于 virtual machine。\n",
    "- `mod`（{class}`~tvm.IRModule`）：包含函数集合的 Relay 模块。\n",
    "- `device`（{class}`Device`）：执行代码的设备。\n",
    "- `target`：任何类似多目标的对象，请参见 {meth}`tvm.target.Target.canon_multi_target`。对于同构（homogeneous）编译，唯一的构建目标（target）。对于异构（heterogeneous）编译，可能的构建目标的字典或列表。注意：虽然此 API 允许多个目标，但它不允许多个设备，因此尚不支持异构编译。\n",
    "- `params`（`dict[str, NDArray]`）：在推理期间不改变的 graph 的输入参数。\n",
    "\n",
    "返回：{class}`tvm.relay.backend.interpreter.Executor`。\n",
    "\n",
    "简单示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tvm.nd.NDArray shape=(1,), cpu(0)>\n",
       "array([3.], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tvm\n",
    "import numpy as np\n",
    "\n",
    "x = tvm.relay.var(\"x\", tvm.relay.TensorType([1], dtype=\"float32\"))\n",
    "expr = tvm.relay.add(x, tvm.relay.Constant(tvm.nd.array(np.array([1], dtype=\"float32\"))))\n",
    "executor = tvm.relay.create_executor(\n",
    "    kind=\"vm\", mod=tvm.IRModule.from_expr(tvm.relay.Function([x], expr))\n",
    ")\n",
    "executor.evaluate()(np.array([2], dtype=\"float32\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 `params` 主要用于：\n",
    "\n",
    "```python\n",
    "def bind_params_by_name(func, params):\n",
    "    \"\"\"Bind params to function by name.\n",
    "    This could be useful when assembling custom Relay optimization\n",
    "    passes that involve constant folding.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : relay.Function\n",
    "        The function to bind parameters to.\n",
    "\n",
    "    params : dict of str to NDArray\n",
    "        Input parameters to the graph that do not change\n",
    "        during inference time. Used for constant folding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    func : relay.Function\n",
    "        The function with parameters bound\n",
    "    \"\"\"\n",
    "    inputs = _convert_param_map(params)\n",
    "    return _build_module.BindParamsByName(func, inputs)\n",
    "\n",
    "...\n",
    "raw_targets = Target.canon_multi_target(target)\n",
    "if mod is None:\n",
    "    mod = IRModule()\n",
    "if device is not None:\n",
    "    assert device.device_type == raw_targets[0].get_target_device_type()\n",
    "else:\n",
    "    # Derive the default device from the first target.\n",
    "    device = _nd.device(raw_targets[0].get_target_device_type(), 0)\n",
    "\n",
    "if params is not None:\n",
    "    mod = IRModule.from_expr(bind_params_by_name(mod[\"main\"], params))\n",
    "```\n",
    "\n",
    "看带有参数的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%input0: Tensor[(1, 3, 10, 10), float32] /* span=aten::_convolution_0.input0:0:0 */, %aten::_convolution_0.weight: Tensor[(6, 3, 7, 7), float32] /* span=aten::_convolution_0.weight:0:0 */) {\n",
      "  %0 = nn.conv2d(%input0, %aten::_convolution_0.weight, padding=[0, 0, 0, 0], channels=6, kernel_size=[7, 7]) /* span=aten::_convolution_0:0:0 */;\n",
      "  nn.softmax(%0, axis=1) /* span=aten::softmax_0:0:0 */\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def assert_shapes_match(tru, est):\n",
    "    \"\"\"Verfiy whether the shapes are equal\"\"\"\n",
    "    if tru.shape != est.shape:\n",
    "        msg = \"Output shapes {} and {} don't match\"\n",
    "        raise AssertionError(msg.format(tru.shape, est.shape))\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "class Conv2D(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(3, 6, 7, bias=False)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return self.softmax(self.conv(args[0]))\n",
    "        \n",
    "input_shape = [1, 3, 10, 10]\n",
    "baseline_model = Conv2D().float().eval()\n",
    "input_data = torch.rand(input_shape).float()\n",
    "baseline_input = [input_data]\n",
    "with torch.no_grad():\n",
    "    baseline_outputs = baseline_model(*[input.clone() for input in baseline_input])\n",
    "if isinstance(baseline_outputs, tuple):\n",
    "    baseline_outputs = tuple(out.cpu().numpy() for out in baseline_outputs)\n",
    "else:\n",
    "    baseline_outputs = (baseline_outputs.cpu().numpy(),)\n",
    "trace = torch.jit.trace(baseline_model, [input.clone() for input in baseline_input])\n",
    "trace = trace.float().eval()\n",
    "input_names = [f\"input{idx}\" for idx, _ in enumerate(baseline_input)]\n",
    "input_shapes = list(zip(input_names, [inp.shape for inp in baseline_input]))\n",
    "input_names = [f\"input{idx}\" for idx, _ in enumerate(baseline_input)]\n",
    "input_shapes = list(zip(input_names, [inp.shape for inp in baseline_input]))\n",
    "mod, params = tvm.relay.frontend.from_pytorch(trace, input_shapes, custom_convert_map=None)\n",
    "print(mod[\"main\"])\n",
    "for arg in mod[\"main\"].params[: len(input_names)]:\n",
    "    assert arg.name_hint in input_names\n",
    "compiled_input = dict(zip(input_names, [inp.clone().cpu().numpy() for inp in baseline_input]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "kind = \"graph\"\n",
    "targets = [\"llvm\"]\n",
    "# targets = [\"llvm\", \"cuda\"]\n",
    "check_correctness = True\n",
    "rtol = 1e-5\n",
    "atol = 1e-5\n",
    "expected_ops = ['nn.conv2d']\n",
    "for target in targets:\n",
    "    if not tvm.runtime.enabled(target):\n",
    "        continue\n",
    "    dev = tvm.device(target, 0)\n",
    "    executor = tvm.relay.create_executor(\n",
    "        kind, mod=mod, device=dev, target=target, params=params\n",
    "    ).evaluate()\n",
    "    result = executor(**compiled_input)\n",
    "    if not isinstance(result, list):\n",
    "        result = [result]\n",
    "\n",
    "    for i, baseline_output in enumerate(baseline_outputs):\n",
    "        output = result[i].asnumpy()\n",
    "        assert_shapes_match(baseline_output, output)\n",
    "        if check_correctness:\n",
    "            np.testing.assert_allclose(baseline_output, output, rtol=rtol, atol=atol)\n",
    "    \n",
    "    def visit(op):\n",
    "        if isinstance(op, tvm.ir.op.Op):\n",
    "            if op.name in expected_ops:\n",
    "                expected_ops.remove(op.name)\n",
    "\n",
    "    tvm.relay.analysis.post_order_visit(mod[\"main\"].body, visit)\n",
    "\n",
    "    if expected_ops:\n",
    "        msg = \"TVM Relay do not contain expected ops {}\"\n",
    "        raise AssertionError(msg.format(expected_ops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
