{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TVM Graph JSON 简介\n",
    "\n",
    "```{admonition} 导航\n",
    "解读 TVM 通过 `tvm/src/relay/backend/graph_executor_codegen.cc` 编译（{func}`tvm.relay.build` 或者 {func}`vta.build`）生成的库 `lib` 保存的计算图信息 `lib.graph_json`。\n",
    "```\n",
    "\n",
    "`lib.graph_json` 包含信息如下：\n",
    "\n",
    "::::{tab-set}\n",
    ":::{tab-item} nodes\n",
    "节点是占位符或可计算节点。`nodes` 存储为列表。节点包含以下信息：\n",
    "\n",
    "- ``op``：运算类型， ``null`` 意味着它是占位符/变量/输入节点，``tvm_op`` 意味着这个节点可以被执行\n",
    "- ``name``：节点名字\n",
    "- ``inputs``：此运算的 inputs 位置，inputs 是包含 (nodeid, index, version) 的元组列表。(可选)\n",
    "- ``attrs``：包含以下信息的节点属性\n",
    "\n",
    "    - ``flatten_data``：是否需要在执行前将数据扁平化（flattened）\n",
    "    - ``func_name``：融合函数名，对应于 Relay 编译过程生成的库中的符号。\n",
    "    - ``num_inputs``：此节点的 `inputs` 个数\n",
    "    - ``num_outputs``：此节点产生的 outputs 个数\n",
    ":::\n",
    ":::{tab-item} arg_nodes\n",
    "参数节点的索引列表，它是计算图的占位符/变量/输入节点 或 constant/param。\n",
    ":::\n",
    ":::{tab-item} heads\n",
    "此运算的输出节点的位置列表。\n",
    ":::\n",
    ":::{tab-item} node_row_ptr\n",
    "存储 forward 路径的历史，所以推断任务中可以跳过某些算子来构建子图。\n",
    ":::\n",
    ":::{tab-item} attrs\n",
    "可以包含版本号或类似的有用信息。\n",
    "\n",
    "- ``storage_id``：存储布局中每个节点的内存 slot id。将参数名称映射到一对 ({`storage_id`: `tvm.runtime.NDArray`})。在运行时，可以使用 `storage_id` 查找参数。\n",
    "- ``dtype``：每个节点的数据类型 (enum 值)。\n",
    "- ``dltype``：每个节点的数据类型按顺序排列。\n",
    "- ``shape``：每个节点的形状 k 阶。\n",
    "- ``device_index``：按顺序为每个节点分配设备。\n",
    ":::\n",
    "::::\n",
    "\n",
    "下面以向量加法为例说明："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "\n",
    "type_annotation = relay.TensorType(shape=(5, 5),\n",
    "                                   dtype=\"float32\")\n",
    "\n",
    "def add(a, b):\n",
    "    add_op = a + b\n",
    "    return relay.Function([a, b],\n",
    "                          add_op,\n",
    "                          ret_type=type_annotation,\n",
    "                          type_params=None)\n",
    "\n",
    "\n",
    "a, b = [relay.var(name, type_annotation) for name in \"ab\"]\n",
    "mod = tvm.IRModule.from_expr(add(a, b))\n",
    "rt_lib = relay.build(mod, target=\"llvm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rt_lib.graph_json` 存储为字符串："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rt_lib.graph_json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看构建的计算图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%a: Tensor[(5, 5), float32], %b: Tensor[(5, 5), float32]) -> Tensor[(5, 5), float32] {\n",
      "  add(%a, %b)\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rt_lib.ir_mod)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看函数元数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tvmgen_default_fused_add\": FunctionInfoNode(\n",
      "workspace_sizes={llvm -keys=cpu : 0},\n",
      "  io_sizes={llvm -keys=cpu : 100},\n",
      "  constant_sizes={llvm -keys=cpu : 0},\n",
      "  tir_primfuncs={llvm -keys=cpu : PrimFunc([p0, p1, T_add]) attrs={\"from_legacy_te_schedule\": (bool)1, \"global_symbol\": \"tvmgen_default_fused_add\", \"tir.noalias\": (bool)1, \"hash\": \"f01462d5c0c6f96c\"} {\n",
      "  parallel (ax0, 0, 5) {\n",
      "    let cse_var_1 = (ax0*5)\n",
      "    T_add[ramp(cse_var_1, 1, 5)] = (p0[ramp(cse_var_1, 1, 5)] + p1[ramp(cse_var_1, 1, 5)])\n",
      "  }\n",
      "}\n",
      "},\n",
      "  relay_primfuncs={llvm -keys=cpu : fn (%p0: Tensor[(5, 5), float32] /* ty=Tensor[(5, 5), float32] */, %p1: Tensor[(5, 5), float32] /* ty=Tensor[(5, 5), float32] */, hash=\"f01462d5c0c6f96c\", prim_funcs={'tvmgen_default_fused_add'=meta[tir.PrimFunc][0]}, target=meta[Target][0], Primitive=1, prim_fn_var='tvmgen_default_fused_add') -> Tensor[(5, 5), float32] {\n",
      "  add(%p0, %p1) /* ty=Tensor[(5, 5), float32] */\n",
      "} /* ty=fn (Tensor[(5, 5), float32], Tensor[(5, 5), float32]) -> Tensor[(5, 5), float32] */\n",
      "}), \"__tvm_main__\": FunctionInfoNode(\n",
      "workspace_sizes={llvm -keys=cpu : 0},\n",
      "  io_sizes={llvm -keys=cpu : 300},\n",
      "  constant_sizes={llvm -keys=cpu : 0},\n",
      "  tir_primfuncs={},\n",
      "  relay_primfuncs={llvm -keys=cpu : fn (%a {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=35b3aa0, kind='llvm', keys={'cpu'}, host=Target(id=35b3990, kind='llvm', keys={'cpu'})))}: Tensor[(5, 5), float32] /* ty=Tensor[(5, 5), float32] */, %b {virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=35b3aa0, kind='llvm', keys={'cpu'}, host=Target(id=35b3990, kind='llvm', keys={'cpu'})))}: Tensor[(5, 5), float32] /* ty=Tensor[(5, 5), float32] */, executor=meta[Executor][0], runtime=meta[Runtime][0], hash=\"4fcdf772a04eb1ba\", virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(id=35b3aa0, kind='llvm', keys={'cpu'}, host=Target(id=35b3990, kind='llvm', keys={'cpu'})))) -> Tensor[(5, 5), float32] {\n",
      "  %0 = fn (%p0: Tensor[(5, 5), float32] /* ty=Tensor[(5, 5), float32] */, %p1: Tensor[(5, 5), float32] /* ty=Tensor[(5, 5), float32] */, Primitive=1, hash=\"f01462d5c0c6f96c\") -> Tensor[(5, 5), float32] {\n",
      "    add(%p0, %p1) /* ty=Tensor[(5, 5), float32] */\n",
      "  } /* ty=fn (Tensor[(5, 5), float32], Tensor[(5, 5), float32]) -> Tensor[(5, 5), float32] */;\n",
      "  %0(%a, %b) /* ty=Tensor[(5, 5), float32] */\n",
      "} /* ty=fn (Tensor[(5, 5), float32], Tensor[(5, 5), float32]) -> Tensor[(5, 5), float32] */\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "print(rt_lib.function_metadata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `toml` 查看可读性更好："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg_nodes = [ 0, 1,]\n",
      "heads = [ [ 2, 0, 0,],]\n",
      "node_row_ptr = [ 0, 1, 2, 3,]\n",
      "[[nodes]]\n",
      "op = \"null\"\n",
      "name = \"a\"\n",
      "inputs = []\n",
      "\n",
      "[[nodes]]\n",
      "op = \"null\"\n",
      "name = \"b\"\n",
      "inputs = []\n",
      "\n",
      "[[nodes]]\n",
      "op = \"tvm_op\"\n",
      "name = \"tvmgen_default_fused_add\"\n",
      "inputs = [ [ 0, 0, 0,], [ 1, 0, 0,],]\n",
      "\n",
      "[nodes.attrs]\n",
      "num_outputs = \"1\"\n",
      "num_inputs = \"2\"\n",
      "flatten_data = \"0\"\n",
      "func_name = \"tvmgen_default_fused_add\"\n",
      "hash = \"f01462d5c0c6f96c\"\n",
      "\n",
      "[attrs]\n",
      "dltype = [ \"list_str\", [ \"float32\", \"float32\", \"float32\",],]\n",
      "device_index = [ \"list_int\", [ 1, 1, 1,],]\n",
      "storage_id = [ \"list_int\", [ 0, 1, 2,],]\n",
      "shape = [ \"list_shape\", [ [ 5, 5,], [ 5, 5,], [ 5, 5,],],]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import toml\n",
    "\n",
    "print(toml.dumps(eval(rt_lib.graph_json)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0af55fbb1c4b4e8ca009f3673b968438b459a89daa1170f52b672ab74da765c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
