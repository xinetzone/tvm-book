{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fake-quantization-to-integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.relay.transform import fake_quantization_to_integer\n",
    "\n",
    "def compare_fq_to_int(expr, args, allow_rounding_error=False):\n",
    "    mod = tvm.IRModule.from_expr(expr)\n",
    "    mod = tvm.relay.transform.InferType()(mod)\n",
    "    mod_int = tvm.relay.transform.FakeQuantizationToInteger()(mod)\n",
    "    assert not tvm.ir.structural_equal(mod, mod_int)\n",
    "    result = (\n",
    "        relay.create_executor(\"vm\", mod=mod, device=tvm.cpu(), target=\"llvm\")\n",
    "        .evaluate()(*args)\n",
    "        .numpy()\n",
    "    )\n",
    "    result_int = (\n",
    "        relay.create_executor(\"vm\", mod=mod_int, device=tvm.cpu(), target=\"llvm\")\n",
    "        .evaluate()(*args)\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    if allow_rounding_error:\n",
    "        assert np.all(np.abs(result.astype(\"int32\") - result_int.astype(\"int32\")) <= 1)\n",
    "    else:\n",
    "        assert np.array_equal(result, result_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fake_quantize_conv():\n",
    "    for out_dtype in [\"int8\", \"uint8\"]:\n",
    "        x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "        w = relay.var(\"w\", shape=[16, 3, 5, 5], dtype=\"int8\")\n",
    "        one = relay.const(1.0)\n",
    "        zero = relay.const(0)\n",
    "\n",
    "        op = relay.op.nn.conv2d(\n",
    "            relay.qnn.op.dequantize(x, relay.const(2.0), zero),\n",
    "            relay.qnn.op.dequantize(w, relay.const(0.5), zero),\n",
    "            kernel_size=[5, 5],\n",
    "        )\n",
    "        op = relay.qnn.op.quantize(op, one, zero, out_dtype=out_dtype)\n",
    "\n",
    "        x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "        w_np = np.random.randint(-128, 127, size=[16, 3, 5, 5], dtype=\"int8\")\n",
    "\n",
    "        compare_fq_to_int(op, [x_np, w_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_conv_per_channel():\n",
    "    for out_dtype in [\"int8\", \"uint8\"]:\n",
    "        x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "        w = relay.var(\"w\", shape=[16, 3, 5, 5], dtype=\"int8\")\n",
    "        one = relay.const([1.0] * 16)\n",
    "        zero_point = relay.const([np.random.randint(0, 255)] * 16)\n",
    "\n",
    "        op = relay.op.nn.conv2d(\n",
    "            relay.qnn.op.dequantize(x, relay.const(2.0), relay.const(0)),\n",
    "            relay.qnn.op.dequantize(\n",
    "                w, relay.const(np.random.random([16]).astype(\"float32\")), zero_point, axis=0\n",
    "            ),\n",
    "            kernel_size=[5, 5],\n",
    "            channels=16,\n",
    "        )\n",
    "        op = relay.qnn.op.quantize(op, relay.const(1.0), relay.const(0), out_dtype=out_dtype)\n",
    "\n",
    "        x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "        w_np = np.random.randint(-128, 127, size=[16, 3, 5, 5], dtype=\"int8\")\n",
    "\n",
    "        compare_fq_to_int(op, [x_np, w_np], allow_rounding_error=True)\n",
    "\n",
    "\n",
    "def test_fake_quantize_transposeconv():\n",
    "    for out_dtype in [\"int8\", \"uint8\"]:\n",
    "        x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "        w = relay.var(\"w\", shape=[3, 16, 5, 5], dtype=\"int8\")\n",
    "        one = relay.const(1.0)\n",
    "        zero = relay.const(0)\n",
    "\n",
    "        op = relay.op.nn.conv2d_transpose(\n",
    "            relay.qnn.op.dequantize(x, relay.const(2.0), zero),\n",
    "            relay.qnn.op.dequantize(w, relay.const(0.5), zero),\n",
    "            kernel_size=[5, 5],\n",
    "            data_layout=\"NCHW\",\n",
    "            kernel_layout=\"IOHW\",\n",
    "        )\n",
    "        op = relay.qnn.op.quantize(op, one, zero, out_dtype=out_dtype)\n",
    "\n",
    "        x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "        w_np = np.random.randint(-128, 127, size=[3, 16, 5, 5], dtype=\"int8\")\n",
    "\n",
    "        compare_fq_to_int(op, [x_np, w_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_dense():\n",
    "    for out_dtype in [\"int8\", \"uint8\"]:\n",
    "        x = relay.var(\"x\", shape=[128, 64], dtype=\"int8\")\n",
    "        w = relay.var(\"w\", shape=[256, 64], dtype=\"int8\")\n",
    "        one = relay.const(1.0)\n",
    "        zero = relay.const(0)\n",
    "\n",
    "        op = relay.op.nn.dense(\n",
    "            relay.qnn.op.dequantize(x, relay.const(2.0), zero),\n",
    "            relay.qnn.op.dequantize(w, relay.const(0.5), zero),\n",
    "        )\n",
    "        op = relay.qnn.op.quantize(op, one, zero, out_dtype=out_dtype)\n",
    "\n",
    "        x_np = np.random.randint(-128, 127, size=[128, 64], dtype=\"int8\")\n",
    "        w_np = np.random.randint(-128, 127, size=[256, 64], dtype=\"int8\")\n",
    "\n",
    "        compare_fq_to_int(op, [x_np, w_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_dense_per_channel():\n",
    "    for out_dtype in [\"int8\", \"uint8\"]:\n",
    "        x = relay.var(\"x\", shape=[128, 64], dtype=\"int8\")\n",
    "        w = relay.var(\"w\", shape=[256, 64], dtype=\"int8\")\n",
    "        one = relay.const(1.0)\n",
    "        zero = relay.const(0)\n",
    "\n",
    "        op = relay.op.nn.dense(\n",
    "            relay.qnn.op.dequantize(x, relay.const(2.0), zero),\n",
    "            relay.qnn.op.dequantize(\n",
    "                w,\n",
    "                relay.const(np.random.random([256]).astype(\"float32\")),\n",
    "                relay.const([0] * 256),\n",
    "                axis=0,\n",
    "            ),\n",
    "            units=256,\n",
    "        )\n",
    "        op = relay.qnn.op.quantize(op, one, zero, out_dtype=out_dtype)\n",
    "\n",
    "        x_np = np.random.randint(-128, 127, size=[128, 64], dtype=\"int8\")\n",
    "        w_np = np.random.randint(-128, 127, size=[256, 64], dtype=\"int8\")\n",
    "\n",
    "        compare_fq_to_int(op, [x_np, w_np], allow_rounding_error=True)\n",
    "\n",
    "\n",
    "def test_fake_quantize_dense_bias():\n",
    "    out_dtype = \"int8\"\n",
    "    x = relay.var(\"x\", shape=[128, 64], dtype=\"int8\")\n",
    "    w = relay.var(\"w\", shape=[256, 64], dtype=\"int8\")\n",
    "    bias = relay.var(\"bias\", shape=[256], dtype=\"int32\")\n",
    "    one = relay.const(1.0)\n",
    "    zero = relay.const(0)\n",
    "    w_scale = np.random.random([256]).astype(\"float32\")\n",
    "\n",
    "    op = relay.op.nn.dense(\n",
    "        relay.qnn.op.dequantize(x, relay.const(2.0), zero),\n",
    "        relay.qnn.op.dequantize(\n",
    "            w,\n",
    "            relay.const(w_scale),\n",
    "            zero,\n",
    "            axis=0,\n",
    "        ),\n",
    "        units=256,\n",
    "    )\n",
    "\n",
    "    op += relay.qnn.op.dequantize(\n",
    "        bias,\n",
    "        relay.const(2.0 * w_scale),\n",
    "        zero,\n",
    "    )\n",
    "\n",
    "    op = relay.qnn.op.quantize(op, one, zero, out_dtype=out_dtype)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[128, 64], dtype=\"int8\")\n",
    "    w_np = np.random.randint(-128, 127, size=[256, 64], dtype=\"int8\")\n",
    "    bias_np = np.random.randint(-128, 127, size=[256], dtype=\"int32\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np, w_np, bias_np], allow_rounding_error=True)\n",
    "\n",
    "\n",
    "def test_fake_quantize_batch_matmul():\n",
    "    for out_dtype in [\"int8\", \"uint8\"]:\n",
    "        x = relay.var(\"x\", shape=[1, 128, 64], dtype=\"int8\")\n",
    "        w = relay.var(\"w\", shape=[1, 256, 64], dtype=\"int8\")\n",
    "        one = relay.const(1.0)\n",
    "        zero = relay.const(0)\n",
    "\n",
    "        op = relay.op.nn.batch_matmul(\n",
    "            relay.qnn.op.dequantize(x, relay.const(2.0), zero),\n",
    "            relay.qnn.op.dequantize(w, relay.const(0.5), zero),\n",
    "        )\n",
    "        op = relay.qnn.op.quantize(op, one, zero, out_dtype=out_dtype)\n",
    "\n",
    "        x_np = np.random.randint(-128, 127, size=[1, 128, 64], dtype=\"int8\")\n",
    "        w_np = np.random.randint(-128, 127, size=[1, 256, 64], dtype=\"int8\")\n",
    "\n",
    "        compare_fq_to_int(op, [x_np, w_np])\n",
    "\n",
    "\n",
    "def test_fake_transpose_quantize_conv():\n",
    "    x = relay.var(\"x\", shape=[1, 224, 224, 3], dtype=\"int8\")\n",
    "    w = relay.var(\"w\", shape=[16, 3, 5, 5], dtype=\"int8\")\n",
    "    one = relay.const(1.0)\n",
    "    zero = relay.const(0)\n",
    "\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    x = relay.transpose(x, [0, 3, 1, 2])\n",
    "    op = relay.op.nn.conv2d(\n",
    "        x, relay.qnn.op.dequantize(w, relay.const(0.5), zero), kernel_size=[5, 5]\n",
    "    )\n",
    "    op = relay.qnn.op.quantize(op, one, zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 224, 224, 3], dtype=\"int8\")\n",
    "    w_np = np.random.randint(-128, 127, size=[16, 3, 5, 5], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np, w_np])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_fake_transpose_quantize_conv_bias_add_per_channel():\n",
    "    x = relay.var(\"x\", shape=[1, 224, 224, 3], dtype=\"int8\")\n",
    "    w = relay.var(\"w\", shape=[16, 3, 5, 5], dtype=\"int8\")\n",
    "    bias = relay.var(\"bias\", shape=[16], dtype=\"int32\")\n",
    "    one = relay.const(1.0)\n",
    "    zero = relay.const(0)\n",
    "    w_scale = (np.random.random([16]).astype(\"float32\") - 0.5) / 10 + 0.5\n",
    "    noise = (np.random.random([16]).astype(\"float32\") - 0.5) * 1e-15\n",
    "    w_zp = relay.const([0] * 16)\n",
    "\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    x = relay.transpose(x, [0, 3, 1, 2])\n",
    "    op = relay.op.nn.conv2d(\n",
    "        x, relay.qnn.op.dequantize(w, relay.const(w_scale), w_zp, axis=0), kernel_size=[5, 5]\n",
    "    )\n",
    "    op = relay.op.nn.bias_add(\n",
    "        op, relay.qnn.op.dequantize(bias, relay.const(2.0 * w_scale + noise), w_zp, axis=0)\n",
    "    )\n",
    "    op = relay.qnn.op.quantize(op, one, zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 224, 224, 3], dtype=\"int8\")\n",
    "    w_np = np.random.randint(-128, 127, size=[16, 3, 5, 5], dtype=\"int8\")\n",
    "    bias_np = np.random.randint(-32768, 32767, size=[16], dtype=\"int32\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np, w_np, bias_np], allow_rounding_error=True)\n",
    "\n",
    "\n",
    "def test_fake_transpose_quantize_conv_bias_add_mismatch():\n",
    "    x = relay.var(\"x\", shape=[1, 224, 224, 3], dtype=\"int8\")\n",
    "    w = relay.var(\"w\", shape=[16, 3, 5, 5], dtype=\"int8\")\n",
    "    bias = relay.var(\"bias\", shape=[16], dtype=\"int32\")\n",
    "    one = relay.const(1.0)\n",
    "    two = relay.const(2.0)\n",
    "    zero = relay.const(0)\n",
    "\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    x = relay.transpose(x, [0, 3, 1, 2])\n",
    "    op = relay.op.nn.conv2d(\n",
    "        x, relay.qnn.op.dequantize(w, relay.const(0.5), zero), kernel_size=[5, 5]\n",
    "    )\n",
    "    op = relay.op.nn.bias_add(op, relay.qnn.op.dequantize(bias, two, zero))\n",
    "    op = relay.qnn.op.quantize(op, one, zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 224, 224, 3], dtype=\"int8\")\n",
    "    w_np = np.random.randint(-128, 127, size=[16, 3, 5, 5], dtype=\"int8\")\n",
    "    bias_np = np.random.randint(-32768, 32767, size=[16], dtype=\"int32\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np, w_np, bias_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_maxpool():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    zero = relay.const(0)\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    op = relay.op.nn.max_pool2d(x, [3, 3])\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fake_transpose_quantize_conv_bias_add(const_bias=True):\n",
    "    x = relay.var(\"x\", shape=[1, 224, 224, 3], dtype=\"int8\")\n",
    "    w = relay.var(\"w\", shape=[16, 3, 5, 5], dtype=\"int8\")\n",
    "    one = relay.const(1.0)\n",
    "    zero = relay.const(0)\n",
    "    if const_bias:\n",
    "        bias = relay.const(np.random.random(16).astype(\"float32\"))\n",
    "    else:\n",
    "        bias = relay.qnn.op.dequantize(relay.var(\"bias\", shape=[16], dtype=\"int32\"), one, zero)\n",
    "\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    x = relay.transpose(x, [0, 3, 1, 2])\n",
    "    op = relay.op.nn.conv2d(\n",
    "        x, relay.qnn.op.dequantize(w, relay.const(0.5), zero), kernel_size=[5, 5]\n",
    "    )\n",
    "    op = relay.op.nn.bias_add(op, bias)\n",
    "    op = relay.qnn.op.quantize(op, one, zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 224, 224, 3], dtype=\"int8\")\n",
    "    w_np = np.random.randint(-128, 127, size=[16, 3, 5, 5], dtype=\"int8\")\n",
    "    bias_np = np.random.randint(-32768, 32767, size=[16], dtype=\"int32\")\n",
    "    args = [x_np, w_np]\n",
    "\n",
    "    if not const_bias:\n",
    "        args.append(bias_np)\n",
    "    compare_fq_to_int(op, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fake_quantize_adaptive_avgpool1d(output_size=[None, 1]):\n",
    "    x = relay.var(\"x\", shape=[1, 128, 768], dtype=\"int8\")\n",
    "\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), relay.const(-12))\n",
    "    op = relay.op.nn.adaptive_avg_pool1d(x, output_size)\n",
    "    op = relay.qnn.op.quantize(op, relay.const(0.5), relay.const(10))\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 128, 768], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_fake_quantize_avgpool():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), relay.const(-12))\n",
    "    op = relay.op.nn.avg_pool2d(x, [3, 3])\n",
    "    op = relay.qnn.op.quantize(op, relay.const(0.5), relay.const(10))\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np], True)\n",
    "\n",
    "\n",
    "def test_fake_quantize_global_avg_pool():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), relay.const(-12))\n",
    "    op = relay.op.nn.global_avg_pool2d(x)\n",
    "    op = relay.qnn.op.quantize(op, relay.const(0.5), relay.const(10))\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np], True)\n",
    "\n",
    "\n",
    "class TestUnaryQNNOp:\n",
    "    def helper_test_fake_quantize_unary_op(self, fp32_op, pos_values=False):\n",
    "        for dtype in [\"int8\", \"uint8\"]:\n",
    "            x = relay.var(\"x\", shape=[1, 3, 3, 3], dtype=dtype)\n",
    "\n",
    "            zero = -128 if dtype == \"int8\" else 0\n",
    "            if pos_values:\n",
    "                # Use a positive range for quanitzed ops that only work on positive values\n",
    "                input_mid_point = relay.const(zero)\n",
    "                output_mid_point = relay.const(zero)\n",
    "            else:\n",
    "                input_mid_point = relay.const(np.random.randint(0, 255) + zero)\n",
    "                output_mid_point = relay.const(np.random.randint(0, 255) + zero)\n",
    "\n",
    "            input_scale = relay.const(np.random.rand())\n",
    "            output_scale = relay.const(np.random.rand())\n",
    "\n",
    "            x = relay.qnn.op.dequantize(x, input_scale, input_mid_point)\n",
    "            op = fp32_op(x)\n",
    "\n",
    "            op = relay.qnn.op.quantize(op, output_scale, output_mid_point, out_dtype=dtype)\n",
    "\n",
    "            x_np = np.random.randint(0 + zero, 255 + zero, size=[1, 3, 3, 3], dtype=dtype)\n",
    "\n",
    "            compare_fq_to_int(op, [x_np], True)\n",
    "\n",
    "    def test_sqrt(self):\n",
    "        self.helper_test_fake_quantize_unary_op(fp32_op=relay.sqrt, pos_values=True)\n",
    "\n",
    "    def test_rsqrt(self):\n",
    "        self.helper_test_fake_quantize_unary_op(fp32_op=relay.rsqrt, pos_values=True)\n",
    "\n",
    "    def test_exp(self):\n",
    "        self.helper_test_fake_quantize_unary_op(fp32_op=relay.exp)\n",
    "\n",
    "    def test_erf(self):\n",
    "        self.helper_test_fake_quantize_unary_op(fp32_op=relay.erf)\n",
    "\n",
    "    def test_sigmoid(self):\n",
    "        self.helper_test_fake_quantize_unary_op(fp32_op=relay.sigmoid)\n",
    "\n",
    "    def test_tanh(self):\n",
    "        self.helper_test_fake_quantize_unary_op(fp32_op=relay.tanh)\n",
    "\n",
    "    def test_log(self):\n",
    "        self.helper_test_fake_quantize_unary_op(fp32_op=relay.log, pos_values=True)\n",
    "\n",
    "\n",
    "def test_fake_quantize_reshape():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    zero = relay.const(0)\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    op = relay.op.reshape(x, [1, 3, -1])\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_image_resize_bilinear():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    zero = relay.const(0)\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    op = relay.image.resize2d(x, size=[4, 4], method=\"linear\")\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np], allow_rounding_error=True)\n",
    "\n",
    "\n",
    "def test_fake_quantize_abs():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    zero = relay.const(0)\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    op = relay.op.abs(x)\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_expand_dims():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    zero = relay.const(0)\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    op = relay.op.expand_dims(x, axis=1)\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_squeeze():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    zero = relay.const(0)\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    op = relay.op.squeeze(x, axis=[0])\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_strided_slice():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    zero = relay.const(0)\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    op = relay.op.strided_slice(x, begin=[0, 0, 0, 0], end=[1, 1, 112, 112])\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_split():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    zero = relay.const(0)\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    op = relay.op.split(x, axis=3, indices_or_sections=2)\n",
    "    op = relay.qnn.op.quantize(op[0], relay.const(2.0), zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "    op = relay.op.split(x, axis=3, indices_or_sections=[56, 112, 168])\n",
    "    op = relay.qnn.op.quantize(op[1], relay.const(2.0), zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_batch_flatten():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    zero = relay.const(0)\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    op = relay.op.nn.batch_flatten(x)\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_transpose_reshape():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    zero = relay.const(0)\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    op = relay.op.transpose(x, [1, 0, 2, 3])\n",
    "    op = relay.op.reshape(op, [3, -1])\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_concat():\n",
    "    zero = relay.const(0)\n",
    "    inputs = []\n",
    "    for i in range(4):\n",
    "        inputs.append(\n",
    "            relay.qnn.op.dequantize(\n",
    "                relay.var(\"x%d\" % i, shape=[1, 4], dtype=\"int8\"), relay.const(i + 0.5), zero\n",
    "            )\n",
    "        )\n",
    "    concat = relay.op.concatenate(inputs, axis=1)\n",
    "    out = relay.qnn.op.quantize(concat, relay.const(3.5), zero)\n",
    "\n",
    "    inputs_np = []\n",
    "    for i in range(4):\n",
    "        inputs_np.append(np.random.randint(-128, 127, size=[1, 4], dtype=\"int8\"))\n",
    "\n",
    "    compare_fq_to_int(out, inputs_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@pytest.mark.parametrize(\"k\", [0, 1, 5])\n",
    "@pytest.mark.parametrize(\"axis\", [0, -1, 1])\n",
    "@pytest.mark.parametrize(\"is_ascend\", [True, False])\n",
    "@pytest.mark.parametrize(\"dtype\", [\"int8\", \"uint8\"])\n",
    "def test_fake_quantize_topk(k, axis, is_ascend, dtype):\n",
    "    x = relay.var(\"x\", shape=[20, 100], dtype=dtype)\n",
    "    zero = relay.const(0)\n",
    "\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    op = relay.topk(x, k, axis, \"values\", is_ascend, \"float32\")\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), zero, out_dtype=dtype)\n",
    "    x_np = np.random.randint(0, 127, size=[20, 100], dtype=dtype)\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_clip():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"uint8\")\n",
    "\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), relay.const(114))\n",
    "    op = relay.op.clip(x, 0, 6)\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), relay.const(114), out_dtype=\"uint8\")\n",
    "\n",
    "    x_np = np.random.randint(0, 255, size=[1, 3, 224, 224], dtype=\"uint8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_clip_per_channel():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"uint8\")\n",
    "\n",
    "    x = relay.qnn.op.dequantize(\n",
    "        x, relay.const([1.0, 2.0, 3.0]), relay.const([96, 114, 128]), axis=1\n",
    "    )\n",
    "    op = relay.op.clip(x, 0, 6)\n",
    "    op = relay.qnn.op.quantize(\n",
    "        op, relay.const([1.0, 2.0, 3.0]), relay.const([96, 114, 128]), out_dtype=\"uint8\", axis=1\n",
    "    )\n",
    "\n",
    "    x_np = np.random.randint(0, 255, size=[1, 3, 224, 224], dtype=\"uint8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_relu():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"uint8\")\n",
    "\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), relay.const(114))\n",
    "    op = relay.op.nn.relu(x)\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), relay.const(114), out_dtype=\"uint8\")\n",
    "\n",
    "    x_np = np.random.randint(0, 255, size=[1, 3, 224, 224], dtype=\"uint8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_mean():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"uint8\")\n",
    "\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), relay.const(114))\n",
    "    op = relay.op.mean(x)\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), relay.const(114), out_dtype=\"uint8\")\n",
    "\n",
    "    x_np = np.random.randint(0, 255, size=[1, 3, 224, 224], dtype=\"uint8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np], allow_rounding_error=True)\n",
    "\n",
    "\n",
    "def test_fake_quantize_relu_per_channel():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"uint8\")\n",
    "\n",
    "    x = relay.qnn.op.dequantize(\n",
    "        x, relay.const([1.0, 2.0, 3.0]), relay.const([96, 114, 128]), axis=1\n",
    "    )\n",
    "    op = relay.op.nn.relu(x)\n",
    "    op = relay.qnn.op.quantize(\n",
    "        op, relay.const([1.0, 2.0, 3.0]), relay.const([96, 114, 128]), out_dtype=\"uint8\", axis=1\n",
    "    )\n",
    "\n",
    "    x_np = np.random.randint(0, 255, size=[1, 3, 224, 224], dtype=\"uint8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_leaky_relu():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"uint8\")\n",
    "\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), relay.const(114))\n",
    "    op = relay.op.nn.leaky_relu(x, 0.1)\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), relay.const(114), out_dtype=\"uint8\")\n",
    "\n",
    "    x_np = np.random.randint(0, 255, size=[1, 3, 224, 224], dtype=\"uint8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np], True)\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"operator\",\n",
    "    [relay.op.add, relay.op.multiply, relay.op.subtract, relay.op.minimum, relay.op.maximum],\n",
    ")\n",
    "def test_fake_quantize_binary(operator):\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(0.1), relay.const(0))\n",
    "\n",
    "    y = relay.var(\"y\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "    y = relay.qnn.op.dequantize(y, relay.const(0.2), relay.const(0))\n",
    "\n",
    "    op = operator(x, y)\n",
    "    if operator == relay.op.multiply:\n",
    "        out_scale = relay.const(20.0)\n",
    "    else:\n",
    "        out_scale = relay.const(0.1)\n",
    "\n",
    "    op = relay.qnn.op.quantize(op, out_scale, relay.const(0), out_dtype=\"int8\")\n",
    "\n",
    "    x_np = np.random.randint(-25, 25, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "    y_np = np.random.randint(-25, 25, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np, y_np])\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"operator\",\n",
    "    [relay.op.add, relay.op.multiply, relay.op.subtract, relay.op.minimum, relay.op.maximum],\n",
    ")\n",
    "def test_fake_quantize_binary_per_channel(operator):\n",
    "    def verify_binary_per_channel(lhs_scale, rhs_scale, lhs_zp, rhs_zp, out_zp, lhs_axis, rhs_axis):\n",
    "        if operator == relay.op.multiply:\n",
    "            out_scale = relay.const(2.0)\n",
    "            rhs_axis = lhs_axis  # TODO: Support different axes for per-channel quantized multiply\n",
    "        else:\n",
    "            out_scale = relay.const(0.1)\n",
    "\n",
    "        x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "        x = relay.qnn.op.dequantize(x, relay.const(lhs_scale), relay.const(lhs_zp), axis=lhs_axis)\n",
    "\n",
    "        y = relay.var(\"y\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "        y = relay.qnn.op.dequantize(y, relay.const(rhs_scale), relay.const(rhs_zp), axis=rhs_axis)\n",
    "\n",
    "        op = operator(x, y)\n",
    "\n",
    "        op = relay.qnn.op.quantize(op, out_scale, relay.const(out_zp), out_dtype=\"int8\")\n",
    "        x_np = np.random.randint(-25, 25, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "        y_np = np.random.randint(-25, 25, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "        compare_fq_to_int(op, [x_np, y_np], allow_rounding_error=True)\n",
    "\n",
    "    # Same axis\n",
    "    verify_binary_per_channel(\n",
    "        lhs_scale=np.random.uniform(1.0, 5.0, 3),\n",
    "        rhs_scale=np.random.uniform(1.0, 5.0, 3),\n",
    "        lhs_zp=0,\n",
    "        rhs_zp=0,\n",
    "        out_zp=0,\n",
    "        lhs_axis=1,\n",
    "        rhs_axis=1,\n",
    "    )\n",
    "    verify_binary_per_channel(\n",
    "        lhs_scale=np.random.uniform(1.0, 5.0, 3),\n",
    "        rhs_scale=np.random.uniform(1.0, 5.0, 3),\n",
    "        lhs_zp=np.random.randint(1, 3),\n",
    "        rhs_zp=np.random.randint(1, 3),\n",
    "        out_zp=0,\n",
    "        lhs_axis=1,\n",
    "        rhs_axis=1,\n",
    "    )\n",
    "    verify_binary_per_channel(\n",
    "        lhs_scale=np.random.uniform(1.0, 5.0, 3),\n",
    "        rhs_scale=np.random.uniform(1.0, 5.0, 3),\n",
    "        lhs_zp=np.random.randint(1, 3),\n",
    "        rhs_zp=np.random.randint(1, 3),\n",
    "        out_zp=np.random.randint(1, 3),\n",
    "        lhs_axis=1,\n",
    "        rhs_axis=1,\n",
    "    )\n",
    "    verify_binary_per_channel(\n",
    "        lhs_scale=np.random.uniform(1.0, 5.0, 224),\n",
    "        rhs_scale=np.random.uniform(1.0, 5.0, 224),\n",
    "        lhs_zp=np.random.randint(1, 3),\n",
    "        rhs_zp=np.random.randint(1, 3),\n",
    "        out_zp=np.random.randint(1, 3),\n",
    "        lhs_axis=-1,\n",
    "        rhs_axis=-1,\n",
    "    )\n",
    "\n",
    "    # Different axes\n",
    "    verify_binary_per_channel(\n",
    "        lhs_scale=np.random.uniform(1.0, 5.0, 224),\n",
    "        rhs_scale=np.random.uniform(1.0, 5.0, 224),\n",
    "        lhs_zp=0,\n",
    "        rhs_zp=0,\n",
    "        out_zp=0,\n",
    "        lhs_axis=2,\n",
    "        rhs_axis=3,\n",
    "    )\n",
    "    verify_binary_per_channel(\n",
    "        lhs_scale=np.random.uniform(1.0, 5.0, 224),\n",
    "        rhs_scale=np.random.uniform(1.0, 5.0, 224),\n",
    "        lhs_zp=np.random.randint(1, 3),\n",
    "        rhs_zp=np.random.randint(1, 3),\n",
    "        out_zp=0,\n",
    "        lhs_axis=2,\n",
    "        rhs_axis=3,\n",
    "    )\n",
    "    verify_binary_per_channel(\n",
    "        lhs_scale=np.random.uniform(1.0, 5.0, 224),\n",
    "        rhs_scale=np.random.uniform(1.0, 5.0, 224),\n",
    "        lhs_zp=np.random.randint(1, 3),\n",
    "        rhs_zp=np.random.randint(1, 3),\n",
    "        out_zp=np.random.randint(1, 3),\n",
    "        lhs_axis=2,\n",
    "        rhs_axis=3,\n",
    "    )\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"operator\",\n",
    "    [\n",
    "        relay.op.add,\n",
    "        relay.op.multiply,\n",
    "        relay.op.subtract,\n",
    "        relay.op.minimum,\n",
    "        relay.op.maximum,\n",
    "    ],\n",
    ")\n",
    "def test_fake_quantize_binary_const(operator):\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(0.1), relay.const(10))\n",
    "\n",
    "    y = relay.const(1.0)\n",
    "\n",
    "    op = operator(x, y)\n",
    "    op = relay.qnn.op.quantize(op, relay.const(0.1), relay.const(10), out_dtype=\"int8\")\n",
    "\n",
    "    x_np = np.random.randint(-25, 25, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_subtract_different_output_zp():\n",
    "    for dtype in [\"uint8\"]:\n",
    "        x = relay.var(\"x\", shape=[1, 128, 128, 3], dtype=dtype)\n",
    "        x = relay.qnn.op.dequantize(x, relay.const(0.1), relay.const(0), axis=1)\n",
    "\n",
    "        y = relay.const(0.5)\n",
    "\n",
    "        op = relay.subtract(x, y)\n",
    "        op = relay.transpose(op, axes=[0, 3, 1, 2])\n",
    "        op = relay.qnn.op.quantize(op, relay.const(0.2), relay.const(128), out_dtype=dtype, axis=1)\n",
    "\n",
    "        x_np = np.random.randint(0, 255, size=[1, 128, 128, 3], dtype=dtype)\n",
    "\n",
    "        compare_fq_to_int(op, [x_np], True)\n",
    "\n",
    "\n",
    "def test_fake_quantize_pad():\n",
    "    x = relay.var(\"x\", shape=[1, 383, 128], dtype=\"int8\")\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(1.0), relay.const(10))\n",
    "    op = relay.op.nn.pad(x, [[0, 0], [0, 1], [0, 0]], 0.0)\n",
    "    op = relay.qnn.op.quantize(op, relay.const(1.0), relay.const(10), out_dtype=\"int8\")\n",
    "\n",
    "    x_np = np.random.randint(-25, 25, size=[1, 383, 128], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_depth_to_space():\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    zero = relay.const(0)\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "    op = relay.op.nn.depth_to_space(x, 4)\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), zero)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=[1, 3, 224, 224], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_max_min():\n",
    "    def run_test_case(partial_func):\n",
    "        x = relay.var(\"x\", shape=[1, 3, 10, 10], dtype=\"int8\")\n",
    "\n",
    "        zero = relay.const(0)\n",
    "        x = relay.qnn.op.dequantize(x, relay.const(2.0), zero)\n",
    "        # To be a little more realistic since max/min will rarely be by themselves\n",
    "        x = relay.op.nn.depth_to_space(x, 4)\n",
    "        op = partial_func(x)\n",
    "        op = relay.qnn.op.quantize(op, relay.const(2.0), zero)\n",
    "\n",
    "        x_np = np.random.randint(-128, 127, size=[1, 3, 10, 10], dtype=\"int8\")\n",
    "        compare_fq_to_int(op, [x_np])\n",
    "\n",
    "    run_test_case(relay.op.max)\n",
    "    run_test_case(relay.op.min)\n",
    "\n",
    "    # Test forwarding kwargs works\n",
    "    run_test_case(lambda x: relay.op.max(x, axis=1))\n",
    "    run_test_case(lambda x: relay.op.min(x, axis=1))\n",
    "\n",
    "\n",
    "def test_fq_avg_pool_conv2d():\n",
    "    dtype = \"uint8\"\n",
    "    shape_x = [1, 4, 24, 24]\n",
    "    shape_w = [8, 4, 1, 1]\n",
    "    x = relay.var(\"x\", shape=shape_x, dtype=dtype)\n",
    "    w = relay.var(\"w\", shape=shape_w, dtype=dtype)\n",
    "    zero = relay.const(0)\n",
    "    one = relay.const(1.0)\n",
    "\n",
    "    # Tested expression.\n",
    "    op0 = relay.qnn.op.dequantize(x, relay.const(0.64), relay.const(2))\n",
    "    op1 = relay.op.nn.avg_pool2d(op0, [3, 3])\n",
    "    op2 = relay.qnn.op.dequantize(w, relay.const(0.5), relay.const(10))\n",
    "    op3 = relay.op.nn.conv2d(op1, op2, kernel_size=[1, 1])\n",
    "    expr = relay.qnn.op.quantize(op3, one, zero, out_dtype=\"uint8\")\n",
    "\n",
    "    x_np = np.random.randint(0, 255, size=shape_x, dtype=dtype)\n",
    "    w_np = np.random.randint(0, 255, size=shape_w, dtype=dtype)\n",
    "    compare_fq_to_int(expr, [x_np, w_np])\n",
    "\n",
    "\n",
    "def test_fq_hard_fail():\n",
    "    @tvm.ir.register_op_attr(\"nn.conv2d\", \"FTVMFakeQuantizationToInteger\", level=11)\n",
    "    def conv2d(expr, type_map):  # pylint: disable=unused-variable\n",
    "        raise NotImplementedError\n",
    "\n",
    "    x = relay.var(\"x\", shape=[1, 3, 224, 224], dtype=\"int8\")\n",
    "    w = relay.var(\"w\", shape=[16, 3, 5, 5], dtype=\"int8\")\n",
    "    one = relay.const(1.0)\n",
    "    zero = relay.const(0)\n",
    "\n",
    "    op = relay.op.nn.conv2d(\n",
    "        relay.qnn.op.dequantize(x, relay.const(2.0), zero),\n",
    "        relay.qnn.op.dequantize(w, relay.const(0.5), zero),\n",
    "        kernel_size=[5, 5],\n",
    "    )\n",
    "    op = relay.qnn.op.quantize(op, one, zero, out_dtype=\"int8\")\n",
    "    mod = tvm.IRModule.from_expr(op)\n",
    "    mod = tvm.relay.transform.InferType()(mod)\n",
    "\n",
    "    mod_int = tvm.relay.transform.FakeQuantizationToInteger(hard_fail=False)(mod)\n",
    "    assert tvm.ir.structural_equal(mod_int, mod)\n",
    "    # Catch a generic exception because the tvm FFI eats the python exception type\n",
    "    with pytest.raises(Exception):\n",
    "        mod_int = tvm.relay.transform.FakeQuantizationToInteger(hard_fail=True)(mod)\n",
    "\n",
    "\n",
    "def compare_expected_fq_qat_to_int(expr, expected_expr, args, allow_rounding_error=False):\n",
    "    mod = tvm.IRModule.from_expr(expr)\n",
    "    mod_def = tvm.relay.transform.InferType()(mod)\n",
    "    mod_int = tvm.relay.transform.FakeQuantizationToInteger(False, True)(mod_def)\n",
    "    mod_exp = tvm.relay.transform.InferType()(tvm.IRModule.from_expr(expected_expr))\n",
    "    assert not tvm.ir.structural_equal(mod, mod_int)\n",
    "    assert tvm.ir.structural_equal(mod_int, mod_exp)\n",
    "    result_def = (\n",
    "        relay.create_executor(\"vm\", mod=mod_def, device=tvm.cpu(), target=\"llvm\")\n",
    "        .evaluate()(*args)\n",
    "        .numpy()\n",
    "    )\n",
    "    result_int = (\n",
    "        relay.create_executor(\"vm\", mod=mod_int, device=tvm.cpu(), target=\"llvm\")\n",
    "        .evaluate()(*args)\n",
    "        .numpy()\n",
    "    )\n",
    "    result_exp = (\n",
    "        relay.create_executor(\"vm\", mod=mod_exp, device=tvm.cpu(), target=\"llvm\")\n",
    "        .evaluate()(*args)\n",
    "        .numpy()\n",
    "    )\n",
    "    if allow_rounding_error:\n",
    "        assert np.all(np.abs(result_def.astype(\"int32\") - result_int.astype(\"int32\")) <= 1)\n",
    "    else:\n",
    "        assert np.array_equal(result_def, result_int)\n",
    "\n",
    "    assert np.array_equal(result_int, result_exp)\n",
    "\n",
    "\n",
    "def test_fq_qat_op_positive_part():\n",
    "    # Only the first operation is converted, since the next operation(\"add\") is not enabled.\n",
    "    shape_x = [1, 4, 2]\n",
    "    shape_w = [1, 4, 2]\n",
    "    a = relay.var(\"a\", shape=shape_x, dtype=\"int8\")\n",
    "    b = relay.var(\"b\", shape=shape_w, dtype=\"int8\")\n",
    "\n",
    "    op0 = relay.qnn.op.dequantize(a, relay.const(2.0), relay.const(0))\n",
    "    op1 = relay.qnn.op.dequantize(b, relay.const(6.0), relay.const(0))\n",
    "    op2 = relay.op.nn.batch_matmul(op0, op1)\n",
    "    op3 = relay.op.add(op2, relay.const(1.0))\n",
    "    expr = relay.op.erf(op3)\n",
    "\n",
    "    op0 = relay.qnn.op.qnn.batch_matmul(\n",
    "        a, b, relay.const(0), relay.const(0), relay.const(2.0), relay.const(6.0)\n",
    "    )\n",
    "    op1 = relay.qnn.op.qnn.dequantize(op0, relay.const(12.0), relay.const(0))\n",
    "    op2 = relay.op.add(op1, relay.const(1.0))\n",
    "    expected_expr = relay.op.erf(op2)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=shape_x, dtype=\"int8\")\n",
    "    w_np = np.random.randint(-128, 127, size=shape_w, dtype=\"int8\")\n",
    "    compare_expected_fq_qat_to_int(expr, expected_expr, [x_np, w_np])\n",
    "\n",
    "\n",
    "def test_fq_qat_negative_all():\n",
    "    # None of the operations are converted, since the first operation(\"add\") is not enabled.\n",
    "    shape_x = [1, 4, 2]\n",
    "    shape_w = [1, 4, 2]\n",
    "    a = relay.var(\"a\", shape=shape_x, dtype=\"int8\")\n",
    "    b = relay.var(\"b\", shape=shape_w, dtype=\"int8\")\n",
    "\n",
    "    op0 = relay.qnn.op.dequantize(a, relay.const(2.0), relay.const(0))\n",
    "    op1 = relay.qnn.op.dequantize(b, relay.const(6.0), relay.const(0))\n",
    "    op2 = relay.op.add(op1, relay.const(1.0))\n",
    "    op3 = relay.op.nn.batch_matmul(op0, op2)\n",
    "    expr = relay.op.erf(op3)\n",
    "\n",
    "    expected_expr = expr\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=shape_x, dtype=\"int8\")\n",
    "    w_np = np.random.randint(-128, 127, size=shape_w, dtype=\"int8\")\n",
    "    compare_expected_fq_qat_to_int(expr, expected_expr, [x_np, w_np])\n",
    "\n",
    "\n",
    "def test_fq_qat_positive_single():\n",
    "    # The single operation is converted.\n",
    "    shape_x = [1, 4, 2]\n",
    "    shape_w = [1, 4, 2]\n",
    "    a = relay.var(\"a\", shape=shape_x, dtype=\"int8\")\n",
    "    b = relay.var(\"b\", shape=shape_w, dtype=\"int8\")\n",
    "\n",
    "    op0 = relay.qnn.op.dequantize(a, relay.const(2.0), relay.const(0))\n",
    "    op1 = relay.qnn.op.dequantize(b, relay.const(6.0), relay.const(0))\n",
    "    expr = relay.op.nn.batch_matmul(op0, op1)\n",
    "\n",
    "    op0 = relay.qnn.op.qnn.batch_matmul(\n",
    "        a, b, relay.const(0), relay.const(0), relay.const(2.0), relay.const(6.0)\n",
    "    )\n",
    "    expected_expr = relay.qnn.op.qnn.dequantize(op0, relay.const(12.0), relay.const(0))\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=shape_x, dtype=\"int8\")\n",
    "    w_np = np.random.randint(-128, 127, size=shape_w, dtype=\"int8\")\n",
    "    compare_expected_fq_qat_to_int(expr, expected_expr, [x_np, w_np])\n",
    "\n",
    "\n",
    "def test_fq_qat_positive_nothing_to_do():\n",
    "    # All operations are converted by the non-QAT pass.\n",
    "    shape_x = [1, 4, 2]\n",
    "    shape_w = [1, 4, 2]\n",
    "    a = relay.var(\"a\", shape=shape_x, dtype=\"int8\")\n",
    "    b = relay.var(\"b\", shape=shape_w, dtype=\"int8\")\n",
    "\n",
    "    op0 = relay.qnn.op.dequantize(a, relay.const(2.0), relay.const(0))\n",
    "    op1 = relay.qnn.op.dequantize(b, relay.const(6.0), relay.const(0))\n",
    "    op2 = relay.op.nn.batch_matmul(op0, op1)\n",
    "    op3 = relay.op.add(op2, relay.const(1.0))\n",
    "    expr = relay.qnn.op.quantize(op3, relay.const(1.0), relay.const(0), out_dtype=\"int8\")\n",
    "\n",
    "    op0 = relay.qnn.op.batch_matmul(\n",
    "        a, b, relay.const(0), relay.const(0), relay.const(2.0), relay.const(6.0)\n",
    "    )\n",
    "    op1 = relay.qnn.op.quantize(\n",
    "        relay.const(1.0), relay.const(12.0), relay.const(0), out_dtype=\"int32\"\n",
    "    )\n",
    "    op2 = relay.op.add(\n",
    "        op0,\n",
    "        op1,\n",
    "    )\n",
    "    expected_expr = relay.qnn.op.requantize(\n",
    "        op2, relay.const(12.0), relay.const(0), relay.const(1.0), relay.const(0), out_dtype=\"int8\"\n",
    "    )\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=shape_x, dtype=\"int8\")\n",
    "    w_np = np.random.randint(-128, 127, size=shape_w, dtype=\"int8\")\n",
    "    compare_expected_fq_qat_to_int(expr, expected_expr, [x_np, w_np])\n",
    "\n",
    "\n",
    "def test_fq_qat_positive_couple():\n",
    "    # Several consecutive operations are converted.\n",
    "    shape_x = [1, 2, 4]\n",
    "    shape_w = [2]\n",
    "    a = relay.var(\"a\", shape=shape_x, dtype=\"int8\")\n",
    "    b = relay.var(\"b\", shape=shape_w, dtype=\"int8\")\n",
    "\n",
    "    op0 = relay.qnn.op.dequantize(a, relay.const(2.0), relay.const(0))\n",
    "    op1 = relay.qnn.op.dequantize(b, relay.const(6.0), relay.const(0))\n",
    "    op2 = relay.op.reshape(op0, (1, 4, 2))\n",
    "    op3 = relay.op.broadcast_to(op1, (2, 2, 2))\n",
    "    op4 = relay.op.nn.batch_matmul(op2, op3)\n",
    "    expr = relay.op.erf(op4)\n",
    "\n",
    "    op0 = relay.op.reshape(a, (1, 4, 2))\n",
    "    op1 = relay.op.broadcast_to(b, (2, 2, 2))\n",
    "    op3 = relay.qnn.op.qnn.batch_matmul(\n",
    "        op0, op1, relay.const(0), relay.const(0), relay.const(2.0), relay.const(6.0)\n",
    "    )\n",
    "    op4 = relay.qnn.op.qnn.dequantize(op3, relay.const(12.0), relay.const(0))\n",
    "    expected_expr = relay.op.erf(op4)\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=shape_x, dtype=\"int8\")\n",
    "    w_np = np.random.randint(-128, 127, size=shape_w, dtype=\"int8\")\n",
    "    compare_expected_fq_qat_to_int(expr, expected_expr, [x_np, w_np])\n",
    "\n",
    "\n",
    "def test_fq_positive_single_arg_part():\n",
    "    # The single-argument operation is converted.\n",
    "    shape_x = [1, 2, 4]\n",
    "    a = relay.var(\"a\", shape=shape_x, dtype=\"int8\")\n",
    "\n",
    "    op0 = relay.qnn.op.dequantize(a, relay.const(2.0), relay.const(0))\n",
    "\n",
    "    op1 = relay.op.reshape(op0, (1, 4, 2))\n",
    "    expr = relay.op.erf(op1)\n",
    "\n",
    "    op0 = relay.op.reshape(a, (1, 4, 2))\n",
    "    op1 = relay.qnn.op.dequantize(op0, relay.const(2.0), relay.const(0))\n",
    "    expected_expr = relay.op.erf(op1)\n",
    "    x_np = np.random.randint(-128, 127, size=shape_x, dtype=\"int8\")\n",
    "    compare_expected_fq_qat_to_int(expr, expected_expr, [x_np])\n",
    "\n",
    "\n",
    "def test_fq_qat_intermediate_infertype():\n",
    "    # Complex conversion of non-QAT and QAT passes that form FakeQuantizationToInteger.\n",
    "    shape_x = [1, 2, 4]\n",
    "    x = relay.var(\"x\", shape=shape_x, dtype=\"float32\")\n",
    "    const_0 = relay.const(np.random.uniform(size=[1, 4, 2]).astype(\"float32\"))\n",
    "\n",
    "    op0 = relay.qnn.op.quantize(x, relay.const(17.0), relay.const(0), out_dtype=\"int8\")\n",
    "    op1 = relay.qnn.op.dequantize(op0, relay.const(17.0), relay.const(0))\n",
    "    op2 = relay.op.reshape(op1, (1, 4, 2))\n",
    "    op3 = relay.qnn.op.quantize(op2, relay.const(10.0), relay.const(0), out_dtype=\"int8\")\n",
    "    op4 = relay.qnn.op.quantize(const_0, relay.const(1.0), relay.const(8), out_dtype=\"int8\")\n",
    "    op5 = relay.qnn.op.dequantize(op3, relay.const(10.0), relay.const(0))\n",
    "    op6 = relay.qnn.op.dequantize(op4, relay.const(4.0), relay.const(9))\n",
    "    op7 = relay.op.nn.batch_matmul(op5, op6)\n",
    "    expr = relay.op.add(op7, relay.const(5.0))\n",
    "\n",
    "    op0 = relay.qnn.op.quantize(x, relay.const(17.0), relay.const(0), out_dtype=\"int8\")\n",
    "    op1 = relay.op.reshape(op0, (1, 4, 2))\n",
    "    op2 = relay.qnn.op.requantize(\n",
    "        op1, relay.const(17.0), relay.const(0), relay.const(10.0), relay.const(0), out_dtype=\"int8\"\n",
    "    )\n",
    "    op3 = relay.qnn.op.quantize(const_0, relay.const(1.0), relay.const(8), out_dtype=\"int8\")\n",
    "    op4 = relay.qnn.op.batch_matmul(\n",
    "        op2, op3, relay.const(0), relay.const(9), relay.const(10.0), relay.const(4.0)\n",
    "    )\n",
    "    op5 = relay.qnn.op.dequantize(op4, relay.const(40.0), relay.const(0))\n",
    "    expected_expr = relay.op.add(op5, relay.const(5.0))\n",
    "\n",
    "    x_np = np.random.randint(-128, 127, size=shape_x, dtype=\"int32\").astype(\"float32\")\n",
    "    compare_expected_fq_qat_to_int(expr, expected_expr, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_take():\n",
    "    x = relay.var(\"x\", shape=[33, 11], dtype=\"int8\")\n",
    "    indices_np = np.random.randint(0, 33, size=[37], dtype=\"int32\")\n",
    "    indices = relay.const(indices_np)\n",
    "\n",
    "    x = relay.qnn.op.dequantize(x, relay.const(2.0), relay.const(114))\n",
    "    op = relay.op.take(x, indices, axis=0)\n",
    "    op = relay.qnn.op.quantize(op, relay.const(2.0), relay.const(114), out_dtype=\"uint8\")\n",
    "\n",
    "    x_np = np.random.randint(-25, 25, size=[33, 11], dtype=\"int8\")\n",
    "\n",
    "    compare_fq_to_int(op, [x_np])\n",
    "\n",
    "\n",
    "def test_fake_quantize_softmax():\n",
    "    shape = [5, 10]\n",
    "    x_ = relay.var(\"x\", shape=shape, dtype=\"int8\")\n",
    "\n",
    "    is_sorted = lambda a: np.all(a[:-1] <= a[1:])\n",
    "\n",
    "    for scale in [1.0, 0.1, 0.01]:\n",
    "        x = relay.qnn.op.dequantize(x_, relay.const(scale), relay.const(0))\n",
    "        op = relay.op.nn.softmax(x, axis=1)\n",
    "        op = relay.qnn.op.quantize(\n",
    "            op, relay.const(1.0 / 256.0), relay.const(-128), out_dtype=\"int8\"\n",
    "        )\n",
    "\n",
    "        x_np = np.random.randint(-128, 127, size=shape, dtype=\"int8\")\n",
    "        x_np = np.sort(x_np)\n",
    "        args = [x_np]\n",
    "\n",
    "        mod = tvm.IRModule.from_expr(op)\n",
    "        mod = tvm.relay.transform.InferType()(mod)\n",
    "        mod_int = tvm.relay.transform.FakeQuantizationToInteger(\n",
    "            hard_fail=True, optional_qnn_ops=[\"nn.softmax\"]\n",
    "        )(mod)\n",
    "        assert not tvm.ir.structural_equal(mod, mod_int)\n",
    "\n",
    "        result = (\n",
    "            relay.create_executor(\"vm\", mod=mod, device=tvm.cpu(), target=\"llvm\")\n",
    "            .evaluate()(*args)\n",
    "            .numpy()\n",
    "        )\n",
    "        result_int = (\n",
    "            relay.create_executor(\"vm\", mod=mod_int, device=tvm.cpu(), target=\"llvm\")\n",
    "            .evaluate()(*args)\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        # Check at least the softmax output is in ascending order,\n",
    "        # since it is difficult to use allclose due to not-so-good accuracy.\n",
    "        for qdq, qop in zip(result, result_int):\n",
    "            assert is_sorted(qdq)\n",
    "            assert is_sorted(qop)\n",
    "\n",
    "        try:\n",
    "            np.testing.assert_allclose(result_int, result, atol=1)\n",
    "        except AssertionError as e:\n",
    "            # To see the difference\n",
    "            print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
