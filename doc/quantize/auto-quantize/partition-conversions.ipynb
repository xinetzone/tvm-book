{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `partition_conversions`\n",
    "\n",
    "{func}`tvm.relay.quantize._partition_conversions.partition_conversions` 将模块划分为输入量化、核心量化推理和输出反量化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm.runtime.vm import VirtualMachine\n",
    "from tvm import relay\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv = nn.Conv2d(3, 16, 3, 1, 1, bias=True)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "def create_model(ishape = (1, 3, 4, 4)):\n",
    "    pt_model = Model().eval().float()\n",
    "    input_shapes = [(\"data\", ishape)]\n",
    "    # script_module = torch.jit.script(pt_model)\n",
    "    # mod, params = relay.frontend.from_pytorch(script_module, input_shapes)\n",
    "    idata = torch.rand(ishape).type(torch.float32)\n",
    "    traced_model = torch.jit.trace(pt_model, idata)\n",
    "    # traced_model 翻译为 TVM 前端模型\n",
    "    mod, params = relay.frontend.from_pytorch(traced_model, input_shapes, \n",
    "                                              use_parser_friendly_name=True)\n",
    "    return mod, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "修改前量化配置：\n",
      "qconfig(nbit_input=8, nbit_weight=8, nbit_activation=32, calibrate_mode=global_scale, global_scale=8, weight_scale=power2, skip_conv_layers==(nullptr), skip_dense_layer==1, do_simulation==0, round_for_shift==1, debug_enabled_ops==(nullptr), rounding==UPWARD, partition_conversions==disabled)\n",
      "当前量化配置：\n",
      "qconfig(nbit_input=8, nbit_weight=8, nbit_activation=32, calibrate_mode=global_scale, global_scale=8, weight_scale=power2, skip_conv_layers==[], skip_dense_layer==1, do_simulation==1, round_for_shift==1, debug_enabled_ops==(nullptr), rounding==UPWARD, partition_conversions==disabled)\n",
      "\n",
      "def @main(%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = relay.op.annotation.simulated_quantize(%data, 0.0625f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, kind=1) /* ty=Tensor[(1, 3, 4, 4), float32] */;\n",
      "  %1 = nn.conv2d(%0, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %2 = add(%1, meta[relay.Constant][1] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %3 = add(%2, meta[relay.Constant][2] /* ty=Tensor[(16, 1, 1), float32] */) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %4 = nn.relu(%3) /* ty=Tensor[(1, 16, 4, 4), float32] span=aten__relu_0:0:0 */;\n",
      "  %5 = relay.op.annotation.simulated_quantize(%4, 0.0625f /* ty=float32 */, -127f /* ty=float32 */, 127f /* ty=float32 */, kind=1) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %6 = annotation.cast_hint(%5, dtype=\"int8\") /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  annotation.stop_fusion(%6) /* ty=Tensor[(1, 16, 4, 4), float32] */\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"修改前量化配置：\\n{relay.quantize.current_qconfig()}\")\n",
    "mod, params = create_model(ishape = (1, 3, 4, 4))\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    with relay.quantize.qconfig(\n",
    "        skip_conv_layers=[],\n",
    "        do_simulation=True\n",
    "    ):\n",
    "        print(f\"当前量化配置：\\n{relay.quantize.current_qconfig()}\\n\")\n",
    "        qmod = relay.quantize.quantize(mod, params)\n",
    "print(qmod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前量化配置：\n",
      "qconfig(nbit_input=8, nbit_weight=8, nbit_activation=32, calibrate_mode=global_scale, global_scale=8, weight_scale=power2, skip_conv_layers==[], skip_dense_layer==1, do_simulation==0, round_for_shift==1, debug_enabled_ops==(nullptr), rounding==UPWARD, partition_conversions==enabled)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod, params = create_model(ishape = (1, 3, 4, 4))\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    with relay.quantize.qconfig(\n",
    "        skip_conv_layers=[],\n",
    "        partition_conversions=\"enabled\",\n",
    "        do_simulation=False\n",
    "    ):\n",
    "        print(f\"当前量化配置：\\n{relay.quantize.current_qconfig()}\\n\")\n",
    "        qmod = relay.quantize.quantize(mod, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @dequantize_outputs(%input: Tensor[(1, 16, 4, 4), int8] /* ty=Tensor[(1, 16, 4, 4), int8] */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  %0 = cast(%input, dtype=\"float32\") /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  multiply(%0, 0.0625f /* ty=float32 */) /* ty=Tensor[(1, 16, 4, 4), float32] */\n",
      "}\n",
      "\n",
      "def @main(%data: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] */) -> Tensor[(1, 16, 4, 4), float32] {\n",
      "  let %quantized_inputs: (Tensor[(1, 3, 4, 4), int8],) /* ty=(Tensor[(1, 3, 4, 4), int8],) */ = @quantize_inputs(%data) /* ty=(Tensor[(1, 3, 4, 4), int8],) */;\n",
      "  %1 = %quantized_inputs.0 /* ty=Tensor[(1, 3, 4, 4), int8] */;\n",
      "  let %quantized_outputs: Tensor[(1, 16, 4, 4), int8] /* ty=Tensor[(1, 16, 4, 4), int8] */ = @quantized_main(%1) /* ty=Tensor[(1, 16, 4, 4), int8] */;\n",
      "  let %dequantized_outputs: Tensor[(1, 16, 4, 4), float32] /* ty=Tensor[(1, 16, 4, 4), float32] */ = @dequantize_outputs(%quantized_outputs) /* ty=Tensor[(1, 16, 4, 4), float32] */;\n",
      "  %dequantized_outputs\n",
      "}\n",
      "\n",
      "def @quantize_inputs(%data1: Tensor[(1, 3, 4, 4), float32] /* ty=Tensor[(1, 3, 4, 4), float32] span=aten___convolution_0_data:0:0 */) -> (Tensor[(1, 3, 4, 4), int8],) {\n",
      "  %2 = multiply(%data1, 16f /* ty=float32 */) /* ty=Tensor[(1, 3, 4, 4), float32] */;\n",
      "  %3 = round(%2) /* ty=Tensor[(1, 3, 4, 4), float32] */;\n",
      "  %4 = clip(%3, a_min=-127f, a_max=127f) /* ty=Tensor[(1, 3, 4, 4), float32] */;\n",
      "  let %data2: Tensor[(1, 3, 4, 4), int8] /* ty=Tensor[(1, 3, 4, 4), int8] */ = cast(%4, dtype=\"int8\") /* ty=Tensor[(1, 3, 4, 4), int8] */;\n",
      "  (%data2,) /* ty=(Tensor[(1, 3, 4, 4), int8],) */\n",
      "}\n",
      "\n",
      "def @quantized_main(%data3: Tensor[(1, 3, 4, 4), int8] /* ty=Tensor[(1, 3, 4, 4), int8] */) -> Tensor[(1, 16, 4, 4), int8] {\n",
      "  %5 = nn.conv2d(%data3, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), int8] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3], out_dtype=\"int32\") /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %6 = add(%5, meta[relay.Constant][1] /* ty=Tensor[(16, 1, 1), int32] */) /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %7 = add(%6, meta[relay.Constant][2] /* ty=Tensor[(16, 1, 1), int32] */) /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %8 = nn.relu(%7) /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %9 = add(%8, 256 /* ty=int32 */) /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %10 = right_shift(%9, 9 /* ty=int32 */) /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %11 = clip(%10, a_min=-127f, a_max=127f) /* ty=Tensor[(1, 16, 4, 4), int32] */;\n",
      "  %12 = cast(%11, dtype=\"int8\") /* ty=Tensor[(1, 16, 4, 4), int8] */;\n",
      "  annotation.stop_fusion(%12) /* ty=Tensor[(1, 16, 4, 4), int8] */\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(qmod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "dev = tvm.cpu()\n",
    "data_np = np.random.uniform(low=-1, high=1, size=[1, 3, 4, 4]).astype(\"float32\")\n",
    "input_dict = {\"data\": data_np}\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    qvm_exec = relay.vm.compile(qmod, target=\"llvm\", params=params)\n",
    "qvm = VirtualMachine(qvm_exec, dev)\n",
    "qvm.set_input(\"main\", **input_dict)\n",
    "tvm_qres = qvm.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
