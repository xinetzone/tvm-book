{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VTA topi.conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "import tvm\n",
    "from tvm import te\n",
    "from tvm import relay\n",
    "from tvm import autotvm\n",
    "from tvm.contrib.utils import tempdir\n",
    "# from tvm.contrib.pickle_memoize import memoize\n",
    "from tvm import topi\n",
    "import tvm.topi.testing\n",
    "import vta\n",
    "import vta.testing\n",
    "from vta.testing import simulator\n",
    "\n",
    "# FIXME: 需要自定义 clip 算子来规避某种模式检测的限制。\n",
    "@tvm.te.tag_scope(tag=topi.tag.ELEMWISE)\n",
    "def my_clip(x, a_min, a_max):\n",
    "    \"\"\"与 topi 当前的 clip 不同，将最小值和最大值分为两个阶段。\"\"\"\n",
    "    const_min = tvm.tir.const(a_min, x.dtype)\n",
    "    const_max = tvm.tir.const(a_max, x.dtype)\n",
    "    x = te.compute(x.shape, lambda *i: tvm.te.min(x(*i), const_max), name=\"clipA\")\n",
    "    x = te.compute(x.shape, lambda *i: tvm.te.max(x(*i), const_min), name=\"clipB\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Workload = namedtuple(\n",
    "    \"Conv2DWorkload\",\n",
    "    [\n",
    "        \"batch\",\n",
    "        \"height\",\n",
    "        \"width\",\n",
    "        \"in_filter\",\n",
    "        \"out_filter\",\n",
    "        \"hkernel\",\n",
    "        \"wkernel\",\n",
    "        \"hpad\",\n",
    "        \"wpad\",\n",
    "        \"hstride\",\n",
    "        \"wstride\",\n",
    "    ],\n",
    ")\n",
    "env = vta.get_env()\n",
    "remote = tvm.rpc.LocalSession()\n",
    "# ResNet18 workloads\n",
    "resnet_wkls = [\n",
    "    # Workloads of resnet18 on imagenet\n",
    "    # ('resnet-18.C1',  Workload(env.BATCH, 224, 224, 3,   64,  7, 7, 3, 3, 2, 2)),\n",
    "    (\"resnet-18.C2\", Workload(env.BATCH, 56, 56, 64, 64, 3, 3, 1, 1, 1, 1)),\n",
    "    # (\"resnet-18.C3\", Workload(env.BATCH, 56, 56, 64, 128, 3, 3, 1, 1, 2, 2)),\n",
    "    # (\"resnet-18.C4\", Workload(env.BATCH, 56, 56, 64, 128, 1, 1, 0, 0, 2, 2)),\n",
    "    # (\"resnet-18.C5\", Workload(env.BATCH, 28, 28, 128, 128, 3, 3, 1, 1, 1, 1)),\n",
    "    # (\"resnet-18.C6\", Workload(env.BATCH, 28, 28, 128, 256, 3, 3, 1, 1, 2, 2)),\n",
    "    # (\"resnet-18.C7\", Workload(env.BATCH, 28, 28, 128, 256, 1, 1, 0, 0, 2, 2)),\n",
    "    # (\"resnet-18.C8\", Workload(env.BATCH, 14, 14, 256, 256, 3, 3, 1, 1, 1, 1)),\n",
    "    # (\"resnet-18.C9\", Workload(env.BATCH, 14, 14, 256, 512, 3, 3, 1, 1, 2, 2)),\n",
    "    # (\"resnet-18.C10\", Workload(env.BATCH, 14, 14, 256, 512, 1, 1, 0, 0, 2, 2)),\n",
    "    # (\"resnet-18.C11\", Workload(env.BATCH, 7, 7, 512, 512, 3, 3, 1, 1, 1, 1)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2DWorkload(batch=1, height=56, width=56, in_filter=64, out_filter=64, hkernel=3, wkernel=3, hpad=1, wpad=1, hstride=1, wstride=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:25:56] /media/pc/data/lxw/ai/tvm/src/tir/transforms/arg_binder.cc:95: Warning: Trying to bind buffer to another one with lower alignment requirement  required_alignment=256, provided_alignment=64\n",
      "2023-09-25 17:25:56.273 INFO load_module /tmp/tmper0fe63q/conv2d.o\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV2D TEST: Time cost = 0.0928819 sec/op, 2.4893 GOPS\n"
     ]
    }
   ],
   "source": [
    "with autotvm.tophub.context(env.target):\n",
    "    for _, wl in resnet_wkls:\n",
    "        print(wl)\n",
    "        assert wl.hpad == wl.wpad\n",
    "        layout = \"NCHW%dn%dc\" % (env.BATCH, env.BLOCK_IN)\n",
    "        conv2d_fcompute = vta.top.conv2d_packed\n",
    "        conv2d_fschedule = vta.top.schedule_conv2d_packed\n",
    "        # Derive shapes depending upon packing\n",
    "        a_shape = (wl.batch, wl.in_filter, wl.height, wl.width)\n",
    "        w_shape = (wl.out_filter, wl.in_filter, wl.hkernel, wl.wkernel)\n",
    "        b_shape = (wl.batch, wl.out_filter, 1, 1)\n",
    "        # data pack\n",
    "        data_shape = (\n",
    "            wl.batch // env.BATCH,\n",
    "            wl.in_filter // env.BLOCK_IN,\n",
    "            wl.height,\n",
    "            wl.width,\n",
    "            env.BATCH,\n",
    "            env.BLOCK_IN,\n",
    "        )\n",
    "        kernel_shape = (\n",
    "            wl.out_filter // env.BLOCK_OUT,\n",
    "            wl.in_filter // env.BLOCK_IN,\n",
    "            wl.hkernel,\n",
    "            wl.wkernel,\n",
    "            env.BLOCK_OUT,\n",
    "            env.BLOCK_IN,\n",
    "        )\n",
    "        bias_shape = (\n",
    "            wl.batch // env.BATCH,\n",
    "            wl.out_filter // env.BLOCK_OUT,\n",
    "            1,\n",
    "            1,\n",
    "            env.BATCH,\n",
    "            env.BLOCK_OUT,\n",
    "        )\n",
    "        data = te.placeholder(data_shape, name=\"data\", dtype=env.inp_dtype)\n",
    "        kernel = te.placeholder(kernel_shape, name=\"kernel\", dtype=env.wgt_dtype)\n",
    "        bias = te.placeholder(bias_shape, name=\"bias\", dtype=env.acc_dtype)\n",
    "        padding = relay.nn.get_pad_tuple2d((wl.hpad, wl.wpad))\n",
    "        # Define base computation schedule\n",
    "        with env.target:\n",
    "            res = conv2d_fcompute(\n",
    "                data, kernel, (wl.hstride, wl.wstride), padding, (1, 1), layout, env.acc_dtype\n",
    "            )\n",
    "            \n",
    "            res = topi.right_shift(res, 8)\n",
    "            res = topi.add(res, bias)\n",
    "            res = my_clip(res, 0, (1 << env.OUT_WIDTH - 1) - 1)\n",
    "            res = topi.cast(res, env.out_dtype)\n",
    "            # Derive base schedule\n",
    "            s = conv2d_fschedule([res])\n",
    "            # print(vta.lower(s, [data, kernel, bias, res], simple_mode=True))\n",
    "        # Derive number of ops\n",
    "        fout_height = (wl.height + 2 * wl.hpad - wl.hkernel) // wl.hstride + 1\n",
    "        fout_width = (wl.width + 2 * wl.wpad - wl.wkernel) // wl.wstride + 1\n",
    "        num_ops = (\n",
    "            2\n",
    "            * wl.batch\n",
    "            * fout_height\n",
    "            * fout_width\n",
    "            * wl.hkernel\n",
    "            * wl.wkernel\n",
    "            * wl.out_filter\n",
    "            * wl.in_filter\n",
    "        )\n",
    "\n",
    "        # @memoize(\"vta.tests.test_benchmark_topi.conv2d.verify_nhwc\")\n",
    "        def get_ref_data():\n",
    "            # derive min max for act, wgt, and bias types (max non inclusive)\n",
    "            a_min, a_max = 0 - (1 << (env.INP_WIDTH - 1)), (1 << (env.INP_WIDTH - 1))\n",
    "            w_min, w_max = 0 - (1 << (env.WGT_WIDTH - 1)), (1 << (env.WGT_WIDTH - 1))\n",
    "            b_min, b_max = 0 - 1 << (env.INP_WIDTH + env.WGT_WIDTH - 2), 1 << (\n",
    "                env.INP_WIDTH + env.WGT_WIDTH - 2\n",
    "            )\n",
    "            a_np = np.random.randint(a_min, a_max, size=a_shape).astype(data.dtype)\n",
    "            w_np = np.random.randint(w_min, w_max, size=w_shape).astype(kernel.dtype)\n",
    "            b_np = np.random.randint(b_min, b_max, size=b_shape).astype(env.acc_dtype)\n",
    "            r_np = tvm.topi.testing.conv2d_nchw_python(\n",
    "                a_np.astype(env.acc_dtype),\n",
    "                w_np.astype(env.acc_dtype),\n",
    "                (wl.hstride, wl.wstride),\n",
    "                wl.hpad,\n",
    "            ).astype(env.acc_dtype)\n",
    "            return a_np, w_np, b_np, r_np\n",
    "\n",
    "        # Data in original format\n",
    "        data_np, kernel_np, bias_np, res_ref = get_ref_data()\n",
    "        # data pack\n",
    "        data_np = data_np.reshape(\n",
    "            wl.batch // env.BATCH,\n",
    "            env.BATCH,\n",
    "            wl.in_filter // env.BLOCK_IN,\n",
    "            env.BLOCK_IN,\n",
    "            wl.height,\n",
    "            wl.width,\n",
    "        ).transpose((0, 2, 4, 5, 1, 3))\n",
    "        kernel_np = kernel_np.reshape(\n",
    "            wl.out_filter // env.BLOCK_OUT,\n",
    "            env.BLOCK_OUT,\n",
    "            wl.in_filter // env.BLOCK_IN,\n",
    "            env.BLOCK_IN,\n",
    "            wl.hkernel,\n",
    "            wl.wkernel,\n",
    "        ).transpose((0, 2, 4, 5, 1, 3))\n",
    "        bias_np = bias_np.reshape(\n",
    "            wl.batch // env.BATCH, wl.out_filter // env.BLOCK_OUT, 1, 1, env.BATCH, env.BLOCK_OUT\n",
    "        )\n",
    "        # build\n",
    "        with vta.build_config(disabled_pass={\"tir.CommonSubexprElimTIR\"}):\n",
    "            mod = vta.build(\n",
    "                s,\n",
    "                [data, kernel, bias, res],\n",
    "                target=tvm.target.Target(env.target, host=env.target_host),\n",
    "                name=\"conv2d\",\n",
    "            )\n",
    "\n",
    "        temp = tempdir()\n",
    "        mod.save(temp.relpath(\"conv2d.o\"))\n",
    "        remote.upload(temp.relpath(\"conv2d.o\"))\n",
    "        f = remote.load_module(\"conv2d.o\")\n",
    "        dev = remote.device(str(env.target))\n",
    "\n",
    "        res_np = np.zeros(topi.utils.get_const_tuple(res.shape)).astype(res.dtype)\n",
    "        data_arr = tvm.nd.array(data_np, dev)\n",
    "        kernel_arr = tvm.nd.array(kernel_np, dev)\n",
    "        bias_arr = tvm.nd.array(bias_np, dev)\n",
    "        res_arr = tvm.nd.array(res_np, dev)\n",
    "        time_f = f.time_evaluator(\"conv2d\", dev, number=4)\n",
    "\n",
    "        simulator.clear_stats()\n",
    "        cost = time_f(data_arr, kernel_arr, bias_arr, res_arr)\n",
    "        stats = simulator.stats()\n",
    "\n",
    "        # 正确性\n",
    "        res_orig = res_arr.numpy()\n",
    "        # data pack\n",
    "        res_orig = res_orig.transpose((0, 4, 1, 5, 2, 3)).reshape(\n",
    "            wl.batch, wl.out_filter, fout_height, fout_width\n",
    "        )\n",
    "        bias_np = bias_np.transpose((0, 4, 1, 5, 2, 3)).reshape(wl.batch, wl.out_filter, 1, 1)\n",
    "        res_ref = res_ref >> env.WGT_WIDTH\n",
    "        res_ref += bias_np\n",
    "        res_ref = np.clip(res_ref, 0, (1 << env.OUT_WIDTH - 1) - 1)\n",
    "        res_ref = res_ref.astype(env.out_dtype)\n",
    "        correct = np.allclose(res_orig, res_ref)\n",
    "\n",
    "        # 打印\n",
    "        gops = (num_ops / cost.mean) / float(10**9)\n",
    "        print(f\"CONV2D TEST: Time cost = {cost.mean:g} sec/op, {gops:g} GOPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
